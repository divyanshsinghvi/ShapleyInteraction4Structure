{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aded25-0d85-485a-8cf3-94a13f2b0e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T20:53:09.451930Z",
     "iopub.status.busy": "2024-07-14T20:53:09.451711Z",
     "iopub.status.idle": "2024-07-14T20:53:11.478986Z",
     "shell.execute_reply": "2024-07-14T20:53:11.478114Z",
     "shell.execute_reply.started": "2024-07-14T20:53:09.451906Z"
    }
   },
   "outputs": [],
   "source": [
    "from experiment_runner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3ae0d4-4208-45ca-879c-04c25a7f19b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T20:53:11.492345Z",
     "iopub.status.busy": "2024-07-14T20:53:11.491694Z",
     "iopub.status.idle": "2024-07-14T20:53:11.498428Z",
     "shell.execute_reply": "2024-07-14T20:53:11.496998Z",
     "shell.execute_reply.started": "2024-07-14T20:53:11.492309Z"
    }
   },
   "outputs": [],
   "source": [
    "self = ExperimentRunner(cuda=True, seq_len=20, model_name = 'bert', method=105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc1dce5-e938-406f-bf3c-eb9cd23b3a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T20:53:12.642033Z",
     "iopub.status.busy": "2024-07-14T20:53:12.641742Z",
     "iopub.status.idle": "2024-07-14T20:54:10.834518Z",
     "shell.execute_reply": "2024-07-14T20:54:10.833666Z",
     "shell.execute_reply.started": "2024-07-14T20:53:12.642013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "self.prepare_data()\n",
    "self.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c01c86cc-6a7e-48c7-be3b-2c892d848f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T23:27:03.524468Z",
     "iopub.status.busy": "2024-07-14T23:27:03.523936Z",
     "iopub.status.idle": "2024-07-14T23:27:15.161400Z",
     "shell.execute_reply": "2024-07-14T23:27:15.160622Z",
     "shell.execute_reply.started": "2024-07-14T23:27:03.524440Z"
    }
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_ROWS = 5\n",
    "f_row = list(self.test.iterrows())[0:NUMBER_OF_ROWS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95c3f7a5-9a7f-497a-a864-b0f067d42dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:09:26.485858Z",
     "iopub.status.busy": "2024-07-14T21:09:26.485614Z",
     "iopub.status.idle": "2024-07-14T21:09:26.499947Z",
     "shell.execute_reply": "2024-07-14T21:09:26.498898Z",
     "shell.execute_reply.started": "2024-07-14T21:09:26.485837Z"
    }
   },
   "outputs": [],
   "source": [
    "row = [x[1] for x in f_row]\n",
    "# row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9359560-ce99-4f91-81bf-a63844b317e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:12:03.157724Z",
     "iopub.status.busy": "2024-07-14T21:12:03.156411Z",
     "iopub.status.idle": "2024-07-14T21:12:03.163343Z",
     "shell.execute_reply": "2024-07-14T21:12:03.162312Z",
     "shell.execute_reply.started": "2024-07-14T21:12:03.157692Z"
    }
   },
   "outputs": [],
   "source": [
    "row_sent = [x[1]['sentence'] for x in f_row]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b4da381-0da7-4159-9cf7-2e3c4a2b3c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:09:26.821473Z",
     "iopub.status.busy": "2024-07-14T21:09:26.820643Z",
     "iopub.status.idle": "2024-07-14T21:09:26.826750Z",
     "shell.execute_reply": "2024-07-14T21:09:26.825821Z",
     "shell.execute_reply.started": "2024-07-14T21:09:26.821447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second_Europan War who perform_ secret black _operations and are pitted_against the Imperial unit \" Calamaty Raven \" .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row[2]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "645d7d78-6a00-491c-9ada-9ff8a3ec9575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:13:28.011162Z",
     "iopub.status.busy": "2024-07-14T21:13:28.010258Z",
     "iopub.status.idle": "2024-07-14T21:13:28.029733Z",
     "shell.execute_reply": "2024-07-14T21:13:28.027932Z",
     "shell.execute_reply.started": "2024-07-14T21:13:28.011137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 11748,  4801,  4360,  1997,  1996, 11686,  1017,  1007,  1010,\n",
      "          4141,  3615,  2000,  2004, 11748,  4801,  4360, 11906,  3523,   102],\n",
      "        [  101,  2207,  1999,  2254,  2249,  1999,  2900,  1010,  2009,  2003,\n",
      "          1996,  2353,  2208,  1999,  1996, 11748,  4801,  4360,  1035,   102],\n",
      "        [  101, 15440,  1996,  2168, 10077,  1997,  8608,  1998,  2613,  1030,\n",
      "          1011,  1030,  2051, 11247,  2004,  2049, 16372,  1010,  1996,   102],\n",
      "        [  101,  1996,  2208,  2211,  2458,  1999,  2230,  1010,  4755,  1035,\n",
      "          2058,  1037,  2312,  4664,  1997,  1996,  2147,  1035,  2589,   102],\n",
      "        [  101,  2096,  2009,  6025,  1996,  3115,  2838,  1997,  1996,  2186,\n",
      "          1010,  2009,  2036,  9601,  3674, 24081,  1010,  2107,  1035,   102]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "encoded_row =  self.tokenizer(row_sent, padding=True,  truncation=True, max_length=self.SEQ_LEN, return_tensors ='pt').input_ids\n",
    "if self.CUDA:\n",
    "    encoded_row = encoded_row.cuda()\n",
    "print(encoded_row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a6bb4abe-2131-4f8c-b2b7-0c42f2b445c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T23:21:15.467641Z",
     "iopub.status.busy": "2024-07-14T23:21:15.467332Z",
     "iopub.status.idle": "2024-07-14T23:21:15.477800Z",
     "shell.execute_reply": "2024-07-14T23:21:15.476675Z",
     "shell.execute_reply.started": "2024-07-14T23:21:15.467619Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "NUMBER_OF_PERM = 10\n",
    "### L and R starts from 0\n",
    "l = 1\n",
    "r = 3\n",
    "perms = generate_perm(NUMBER_OF_PERM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_perm(nperm):\n",
    "    perm_stored=[]\n",
    "    arg = np.arange(self.SEQ_LEN)\n",
    "    i = 0\n",
    "    while i < nperm: \n",
    "        perm_stored.append(np.random.permutation(arg))\n",
    "        perm_stored.append(perm_stored[-1][::-1])\n",
    "        i += 1\n",
    "    return np.array(perm_stored)\n",
    "\n",
    "\n",
    "def generate_incremental_masks(permutations, l, r):\n",
    "    n_rows, n_cols = permutations.shape\n",
    "    \n",
    "    masks = np.zeros((n_rows, n_cols), dtype=np.int8)\n",
    "    \n",
    "    stop_mask = (permutations == l) | (permutations == r)\n",
    "    stop_indices = np.argmax(stop_mask, axis=1)\n",
    "    row_indices = np.arange(n_rows)\n",
    "    col_indices = np.arange(n_cols)[np.newaxis, :]\n",
    "    valid_mask = col_indices < stop_indices[:, np.newaxis]\n",
    "    indices = permutations * valid_mask\n",
    "\n",
    "    \n",
    "    for inde, row in enumerate(indices):\n",
    "        non_zero_indices = row[row != 0]\n",
    "        masks[inde, non_zero_indices] = True\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822408b-58df-482a-9f69-2707f5b7d774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T23:53:17.989047Z",
     "iopub.status.busy": "2024-07-14T23:53:17.988493Z",
     "iopub.status.idle": "2024-07-14T23:53:19.889167Z",
     "shell.execute_reply": "2024-07-14T23:53:19.888292Z",
     "shell.execute_reply.started": "2024-07-14T23:53:17.989020Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_smax(encoded_row, masks_tensor):\n",
    "    expanded_input = encoded_row.unsqueeze(0).expand(masks_tensor.size(0), -1, -1)\n",
    "    masks_tensor_expanded = masks_tensor.unsqueeze(1).expand(-1, encoded_row.shape[0], -1)\n",
    "    masked_tensors = torch.where(masks_tensor_expanded == 1, expanded_input, torch.tensor(self.tokenizer.pad_token_id, device='cuda:0'))\n",
    "\n",
    "    masked_tensors = masked_tensors.flatten(start_dim = 0, end_dim = 1)\n",
    "    # masked_tensors = masked_tensors.reshape((masked_tensors.shape[0] * masked_tensors.shape[1], -1))\n",
    "\n",
    "    \n",
    "    masks_tensor_expanded = masks_tensor_expanded.flatten(start_dim = 0, end_dim = 1)\n",
    "    # masks_tensor_expanded = masks_tensor_expanded.reshape((masks_tensor_expanded.shape[0] * masks_tensor_expanded.shape[1], -1))\n",
    "    logits = self.model(masked_tensors, attention_mask=masks_tensor_expanded).logits    \n",
    "    smax = logits.softmax(dim=-1)\n",
    "    return smax, logits\n",
    "    \n",
    "\n",
    "masks = generate_incremental_masks(perms, l, r)\n",
    "masks_tensor = torch.tensor(masks, dtype=torch.long, device='cuda:0')\n",
    "\n",
    "smax_ab, logits_ab = get_smax(encoded_row, masks_tensor.clone())\n",
    "\n",
    "masks_tensor_A = masks_tensor.clone()\n",
    "masks_tensor_A[:, l] = 1\n",
    "smax_a, logits_a = get_smax(encoded_row, masks_tensor_A)\n",
    "\n",
    "masks_tensor_B = masks_tensor.clone()\n",
    "masks_tensor_B[:, r] = 1\n",
    "smax_b, logits_b = get_smax(encoded_row, masks_tensor_B)\n",
    "\n",
    "masks_tensor_phi = masks_tensor.clone()\n",
    "masks_tensor_phi[:, r] = 1\n",
    "masks_tensor_phi[:, l] = 1\n",
    "smax_phi, logits_phi = get_smax(encoded_row, masks_tensor_phi)\n",
    "\n",
    "\n",
    "smax = smax_ab - smax_a - smax_b + smax_phi\n",
    "logits = logits_ab - logits_a - logits_b + logits_phi\n",
    "\n",
    "\n",
    "smax = smax.unflatten(0, (smax.shape[0]//encoded_row.shape[0], encoded_row.shape[0]))\n",
    "logits = logits.unflatten(0, (logits.shape[0]//encoded_row.shape[0], encoded_row.shape[0]))\n",
    "print(smax.shape, logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "32ddc103-f8c9-428c-9df6-5f158278acdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T00:01:58.765932Z",
     "iopub.status.busy": "2024-07-15T00:01:58.765481Z",
     "iopub.status.idle": "2024-07-15T00:01:58.772781Z",
     "shell.execute_reply": "2024-07-15T00:01:58.771731Z",
     "shell.execute_reply.started": "2024-07-15T00:01:58.765910Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm(smax,dim=-1).mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39768c-8475-49b3-9e26-ba7d535bd4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b48efd7a-3d74-433f-844c-34a24b423c68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:39:49.412406Z",
     "iopub.status.busy": "2024-07-14T21:39:49.411912Z",
     "iopub.status.idle": "2024-07-14T21:39:49.439704Z",
     "shell.execute_reply": "2024-07-14T21:39:49.438782Z",
     "shell.execute_reply.started": "2024-07-14T21:39:49.412383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102]],\n",
       "\n",
       "        [[    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102]],\n",
       "\n",
       "        [[    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102],\n",
       "         [    0,     0,     0,  ...,     0,     0,   102]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[    0, 11748,  4801,  ..., 11906,  3523,   102],\n",
       "         [    0,  2207,  1999,  ...,  4360,  1035,   102],\n",
       "         [    0, 15440,  1996,  ...,  1010,  1996,   102],\n",
       "         [    0,  1996,  2208,  ...,  1035,  2589,   102],\n",
       "         [    0,  2096,  2009,  ...,  2107,  1035,   102]],\n",
       "\n",
       "        [[    0, 11748,  4801,  ..., 11906,  3523,   102],\n",
       "         [    0,  2207,  1999,  ...,  4360,  1035,   102],\n",
       "         [    0, 15440,  1996,  ...,  1010,  1996,   102],\n",
       "         [    0,  1996,  2208,  ...,  1035,  2589,   102],\n",
       "         [    0,  2096,  2009,  ...,  2107,  1035,   102]],\n",
       "\n",
       "        [[  101, 11748,  4801,  ..., 11906,  3523,   102],\n",
       "         [  101,  2207,  1999,  ...,  4360,  1035,   102],\n",
       "         [  101, 15440,  1996,  ...,  1010,  1996,   102],\n",
       "         [  101,  1996,  2208,  ...,  1035,  2589,   102],\n",
       "         [  101,  2096,  2009,  ...,  2107,  1035,   102]]], device='cuda:0')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply all masks at once\n",
    "\n",
    "masked_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "453a385c-7dd8-4e9c-8814-9586530de61d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:40:20.044685Z",
     "iopub.status.busy": "2024-07-14T21:40:20.044177Z",
     "iopub.status.idle": "2024-07-14T21:40:20.050068Z",
     "shell.execute_reply": "2024-07-14T21:40:20.049146Z",
     "shell.execute_reply.started": "2024-07-14T21:40:20.044658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9a08f20a-853c-4fb3-a283-f612723c5674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:40:59.243275Z",
     "iopub.status.busy": "2024-07-14T21:40:59.242757Z",
     "iopub.status.idle": "2024-07-14T21:40:59.249382Z",
     "shell.execute_reply": "2024-07-14T21:40:59.248323Z",
     "shell.execute_reply.started": "2024-07-14T21:40:59.243250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f47a96-6f05-4863-b830-a9d8ae8911e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:33:42.224493Z",
     "iopub.status.busy": "2024-07-09T00:33:42.224243Z",
     "iopub.status.idle": "2024-07-09T00:33:45.755366Z",
     "shell.execute_reply": "2024-07-09T00:33:45.754448Z",
     "shell.execute_reply.started": "2024-07-09T00:33:42.224472Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b29a6e-09ee-4163-ae2d-1dff74c927a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:33:42.224493Z",
     "iopub.status.busy": "2024-07-09T00:33:42.224243Z",
     "iopub.status.idle": "2024-07-09T00:33:45.755366Z",
     "shell.execute_reply": "2024-07-09T00:33:45.754448Z",
     "shell.execute_reply.started": "2024-07-09T00:33:42.224472Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba04771-8858-4d10-a6ef-f15a84e0f473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:33:42.224493Z",
     "iopub.status.busy": "2024-07-09T00:33:42.224243Z",
     "iopub.status.idle": "2024-07-09T00:33:45.755366Z",
     "shell.execute_reply": "2024-07-09T00:33:45.754448Z",
     "shell.execute_reply.started": "2024-07-09T00:33:42.224472Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8258f8-a89c-4fcf-a195-67c579fd3889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:33:42.224493Z",
     "iopub.status.busy": "2024-07-09T00:33:42.224243Z",
     "iopub.status.idle": "2024-07-09T00:33:45.755366Z",
     "shell.execute_reply": "2024-07-09T00:33:45.754448Z",
     "shell.execute_reply.started": "2024-07-09T00:33:42.224472Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5be82-d8ec-4728-b007-32c8b42ebc9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:33:42.224493Z",
     "iopub.status.busy": "2024-07-09T00:33:42.224243Z",
     "iopub.status.idle": "2024-07-09T00:33:45.755366Z",
     "shell.execute_reply": "2024-07-09T00:33:45.754448Z",
     "shell.execute_reply.started": "2024-07-09T00:33:42.224472Z"
    }
   },
   "outputs": [],
   "source": [
    "row = list(self.test.iterrows())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8a92825-4c01-40f8-b5b4-4255c1371c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:52:03.434292Z",
     "iopub.status.busy": "2024-07-14T12:52:03.433592Z",
     "iopub.status.idle": "2024-07-14T12:52:03.440112Z",
     "shell.execute_reply": "2024-07-14T12:52:03.439055Z",
     "shell.execute_reply.started": "2024-07-14T12:52:03.434267Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_row =  self.tokenizer(row['sentence'], padding=False,  truncation=True, max_length=self.SEQ_LEN, return_tensors ='pt').input_ids.cuda().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3842bf2-566b-4ced-a3ef-b91b84b3c43f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T12:52:04.377289Z",
     "iopub.status.busy": "2024-07-14T12:52:04.376717Z",
     "iopub.status.idle": "2024-07-14T12:52:04.580709Z",
     "shell.execute_reply": "2024-07-14T12:52:04.579342Z",
     "shell.execute_reply.started": "2024-07-14T12:52:04.377267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -8.5790,  -8.7265,  -8.6121,  ...,  -7.8080,  -7.6920,  -4.1057],\n",
       "         [ -8.9574,  -8.9489,  -8.9837,  ...,  -8.1245,  -8.0452,  -5.1740],\n",
       "         [ -6.6033,  -6.7179,  -6.5225,  ...,  -4.8117,  -5.6250,  -2.1881],\n",
       "         ...,\n",
       "         [-11.6282, -12.2846, -11.9921,  ..., -11.4965, -12.0546,  -8.5730],\n",
       "         [-12.9916, -13.0614, -13.1172,  ..., -11.6188, -12.3721,  -7.7752],\n",
       "         [-14.7684, -14.7193, -14.6344,  ..., -14.1674, -12.2202, -10.8975]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.model(encoded_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e624cf-9606-4c1a-b232-729c27cc8a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:44:20.989110Z",
     "iopub.status.busy": "2024-07-09T00:44:20.988617Z",
     "iopub.status.idle": "2024-07-09T00:44:21.155271Z",
     "shell.execute_reply": "2024-07-09T00:44:21.154035Z",
     "shell.execute_reply.started": "2024-07-09T00:44:20.989085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c15c667-595b-4f99-9411-250f4280ed5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T00:47:07.583245Z",
     "iopub.status.busy": "2024-07-09T00:47:07.582651Z",
     "iopub.status.idle": "2024-07-09T00:47:07.589894Z",
     "shell.execute_reply": "2024-07-09T00:47:07.588934Z",
     "shell.execute_reply.started": "2024-07-09T00:47:07.583224Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,  4801,     0,  1997,     0, 11686,  1017,     0,  1010,\n",
       "          4141,     0,  2000,  2004, 11748,     0,  4360,     0,     0,   102]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5ba5deb2-2bda-4418-b2ca-f975ac5a378e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T13:07:28.538155Z",
     "iopub.status.busy": "2024-07-14T13:07:28.537676Z",
     "iopub.status.idle": "2024-07-14T13:07:28.716774Z",
     "shell.execute_reply": "2024-07-14T13:07:28.715290Z",
     "shell.execute_reply.started": "2024-07-14T13:07:28.538130Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPermutationExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_row\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_permutation.py:82\u001b[0m, in \u001b[0;36mPermutation.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, max_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     79\u001b[0m              outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_explainer.py:266\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 266\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    271\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/explainers/_permutation.py:140\u001b[0m, in \u001b[0;36mPermutation.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m    137\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# evaluate the masked model\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     row_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(fm),) \u001b[38;5;241m+\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/utils/_masked_model.py:64\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m         full_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(masks \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     63\u001b[0m         _convert_delta_mask_to_full(masks, full_masks)\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_masking_call(masks, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/utils/_masked_model.py:144\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[1;32m    141\u001b[0m         all_masked_inputs[i]\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[1;32m    143\u001b[0m joined_masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([np\u001b[38;5;241m.\u001b[39mconcatenate(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m all_masked_inputs])\n\u001b[0;32m--> 144\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjoined_masked_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m _assert_output_input_match(joined_masked_inputs, outputs)\n\u001b[1;32m    146\u001b[0m all_outputs\u001b[38;5;241m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/shap/models/_model.py:26\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 26\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m safe_isinstance(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m is_tensor \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1358\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1358\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1373\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:969\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "def masker(binary_mask, X):\n",
    "    X[binary_mask] = self.tokenizer.pad_token_id\n",
    "    X = X.reshape(1, -1)\n",
    "    return X\n",
    "\n",
    "print(type(self.model))\n",
    "shap.PermutationExplainer(self.model, masker, seed=42)(encoded_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683ccd6-6402-4839-be64-a698ba647f24",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.831405Z",
     "iopub.status.idle": "2024-07-09T00:33:46.831740Z",
     "shell.execute_reply": "2024-07-09T00:33:46.831607Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.831594Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train =  self.tokenizer(self.test['sentence'].to_list(), padding=True,  truncation=True, max_length=self.SEQ_LEN, return_tensors ='pt').input_ids\n",
    "# X_train =  self.tokenizer(self.test['sentence'].to_list(), padding=True,  truncation=True, max_length=self.SEQ_LEN, return_tensors ='np').input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990ec7a-44ad-44e3-9940-1c585e0949a5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.833079Z",
     "iopub.status.idle": "2024-07-09T00:33:46.833394Z",
     "shell.execute_reply": "2024-07-09T00:33:46.833257Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.833242Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cdf65-20e3-4210-b87a-e7652cab7aee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.834570Z",
     "iopub.status.idle": "2024-07-09T00:33:46.834900Z",
     "shell.execute_reply": "2024-07-09T00:33:46.834763Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.834750Z"
    }
   },
   "outputs": [],
   "source": [
    "    # vfunc = nshap.vfunc.interventional_shap(lambda x : self.model(x).logits, X_train, target=None, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea8fcc-e3a2-42a2-bfcc-9c9dde700733",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.837066Z",
     "iopub.status.idle": "2024-07-09T00:33:46.837633Z",
     "shell.execute_reply": "2024-07-09T00:33:46.837389Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.837369Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_row =  self.tokenizer(row['sentence'], padding=False,  truncation=True, max_length=self.SEQ_LEN, return_tensors ='pt').input_ids\n",
    "\n",
    "self.model(encoded_row.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0f7f3-12af-43c2-bd49-0a1416b4b882",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.840113Z",
     "iopub.status.idle": "2024-07-09T00:33:46.840573Z",
     "shell.execute_reply": "2024-07-09T00:33:46.840370Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.840349Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_row.reshape(1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf74e1-4870-4dad-8895-2d10a7b47cf6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.843201Z",
     "iopub.status.idle": "2024-07-09T00:33:46.843572Z",
     "shell.execute_reply": "2024-07-09T00:33:46.843419Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.843405Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_row =  self.tokenizer(row['sentence'], padding=False,  truncation=True, max_length=self.SEQ_LEN, return_tensors ='np').input_ids\n",
    "\n",
    "print(encoded_row.shape)\n",
    "# if self.CUDA:\n",
    "#     encoded_row = encoded_row.cuda()\n",
    "if len(row['weak_mwe']) != 0:\n",
    "    vfunc = nshap.vfunc.interventional_shap(lambda x : self.model(x).logits, X_train, target=None, num_samples=1000)\n",
    "    # vfunc = interventional_shap(lambda x : self.model(x).logits, X_train, target=None, num_samples=1000)\n",
    "    shapley_taylor = nshap.shapley_taylor(encoded_row, vfunc, n=2)\n",
    "    print(shapley_taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1134fe5-ce48-4ccc-89d8-30530345a453",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-09T00:33:46.844674Z",
     "iopub.status.idle": "2024-07-09T00:33:46.845043Z",
     "shell.execute_reply": "2024-07-09T00:33:46.844895Z",
     "shell.execute_reply.started": "2024-07-09T00:33:46.844880Z"
    }
   },
   "outputs": [],
   "source": [
    "self.CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016acdb-845e-4c37-a6ae-9f6264e29eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
