{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c48999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 02:48:19.571745: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ThirdParty-8/platforms/linux64Gcc/gperftools-svn/lib:/opt/openfoam8/platforms/linux64GccDPInt32Opt/lib/openmpi-system:/opt/ThirdParty-8/platforms/linux64GccDPInt32/lib/openmpi-system:/usr/lib/x86_64-linux-gnu/openmpi/lib:/home/dsinghvi/OpenFOAM/dsinghvi-8/platforms/linux64GccDPInt32Opt/lib:/opt/site/8/platforms/linux64GccDPInt32Opt/lib:/opt/openfoam8/platforms/linux64GccDPInt32Opt/lib:/opt/ThirdParty-8/platforms/linux64GccDPInt32/lib:/opt/openfoam8/platforms/linux64GccDPInt32Opt/lib/dummy\n",
      "2023-09-10 02:48:19.571774: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('../data_processing/')\n",
    "import pandas as pd\n",
    "from parse_dep import *\n",
    "import re\n",
    "from ast import literal_eval\n",
    "import tokenizations\n",
    "from datasets import load_dataset\n",
    "pipeline = get_spacy_pipeline('bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf18eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bert.pred.mwe', sep='\\t', names=[0, 'sentence', 'd'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a4b81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>d</th>\n",
       "      <th>toks</th>\n",
       "      <th>tags</th>\n",
       "      <th>_</th>\n",
       "      <th>~</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sent</th>\n",
       "      <th>tokens_to_map</th>\n",
       "      <th>token_map</th>\n",
       "      <th>token_map_dict</th>\n",
       "      <th>weak_mwe</th>\n",
       "      <th>strong_mwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>His career was halted in 1997 when he was diag...</td>\n",
       "      <td>{'toks': [['His', 'PRON'], ['career', 'NOUN'],...</td>\n",
       "      <td>[[His, PRON], [career, NOUN], [was, AUX], [hal...</td>\n",
       "      <td>[O, O, O, O-     social, O, O, O, O, O, O-cogn...</td>\n",
       "      <td>[[14, 15]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[his, career, wa, halt, in, 1997, when, he, wa...</td>\n",
       "      <td>[His, career, was, halted, in, 1997, when, he,...</td>\n",
       "      <td>His career was halted in 1997 when he was diag...</td>\n",
       "      <td>[his, career, was, halted, in, 1997, when, he,...</td>\n",
       "      <td>[[0], [1], [2], [3], [4], [5], [6], [7], [8], ...</td>\n",
       "      <td>{0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...</td>\n",
       "      <td>[[16, 17, 18, 19, 20, 21, 22, 23]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  His career was halted in 1997 when he was diag...   \n",
       "\n",
       "                                                   d  \\\n",
       "0  {'toks': [['His', 'PRON'], ['career', 'NOUN'],...   \n",
       "\n",
       "                                                toks  \\\n",
       "0  [[His, PRON], [career, NOUN], [was, AUX], [hal...   \n",
       "\n",
       "                                                tags           _   ~  \\\n",
       "0  [O, O, O, O-     social, O, O, O, O, O, O-cogn...  [[14, 15]]  []   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [his, career, wa, halt, in, 1997, when, he, wa...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [His, career, was, halted, in, 1997, when, he,...   \n",
       "\n",
       "                                                sent  \\\n",
       "0  His career was halted in 1997 when he was diag...   \n",
       "\n",
       "                                       tokens_to_map  \\\n",
       "0  [his, career, was, halted, in, 1997, when, he,...   \n",
       "\n",
       "                                           token_map  \\\n",
       "0  [[0], [1], [2], [3], [4], [5], [6], [7], [8], ...   \n",
       "\n",
       "                                      token_map_dict  \\\n",
       "0  {0: [0], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5...   \n",
       "\n",
       "                             weak_mwe strong_mwe  \n",
       "0  [[16, 17, 18, 19, 20, 21, 22, 23]]         []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{0: [0],\n",
       " 1: [1],\n",
       " 2: [2],\n",
       " 3: [3],\n",
       " 4: [4],\n",
       " 5: [5],\n",
       " 6: [6],\n",
       " 7: [7],\n",
       " 8: [8],\n",
       " 9: [9],\n",
       " 10: [10],\n",
       " 11: [11],\n",
       " 12: [12, 13, 14],\n",
       " 13: [15, 16, 17, 18],\n",
       " 14: [19, 20, 21, 22],\n",
       " 15: [23]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_g(l: list):\n",
    "    return [i.replace(\"Ä \", \"\") for i in l]\n",
    "\n",
    "def list_to_index_dict(input_list):                                                                                                               \n",
    "    index_dict = {}\n",
    "\n",
    "    for index, sublist in enumerate(input_list):\n",
    "        for item in sublist:\n",
    "            if item in index_dict:\n",
    "                index_dict[item].append(index)\n",
    "            else:\n",
    "                index_dict[item] = [index]\n",
    "\n",
    "    return index_dict\n",
    "\n",
    "def map_mwes_together(x, mwe_type):\n",
    "    \"\"\"\n",
    "    ~: for strong\n",
    "    _: for weak\n",
    "    \"\"\"\n",
    "    assert mwe_type in [\"~\", \"_\"], \"MWE Type must be one of [~, _]\"\n",
    "    mapped_mwes = []\n",
    "    for j, mwe in enumerate(x[mwe_type]):\n",
    "        mapped_mwes.append([])\n",
    "        for index in mwe:\n",
    "            \n",
    "            if index-1 in x['token_map_dict']:\n",
    "                for val in x['token_map_dict'][index-1]:\n",
    "                    mapped_mwes[j].append(val+1)\n",
    "                \n",
    "                \n",
    "    return mapped_mwes\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame([[\"His career was halted in 1997 when he was diagnosed with Non @-@ Hodgkin_lymphoma .\", {\"toks\": [[\"His\", \"PRON\"], [\"career\", \"NOUN\"],        [\"was\", \"AUX\"], [\"halted\", \"VERB\"], [\"in\", \"ADP\"], [\"1997\", \"NUM\"], [\"when\", \"SCONJ\"], [\"he\", \"PRON\"], [\"was\", \"AUX\"], [\"diagnosed\", \"VERB\"],     [\"with\", \"ADP\"], [\"Non\", \"PROPN\"], [\"@-@\", \"PROPN\"], [\"Hodgkin\", \"PROPN\"], [\"lymphoma\", \"NOUN\"], [\".\", \"PUNCT\"]], \"tags\": [\"O\", \"O\", \"O\", \"O-     social\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O-cognition\", \"O\", \"O\", \"O\", \"B\", \"\\u012a\", \"O\"], \"_\": [[14, 15]], \"~\": [], \"lemmas\": [\"his\", \"career\", \"wa\",  \"halt\", \"in\", \"1997\", \"when\", \"he\", \"wa\", \"diagnose\", \"with\", \"non\", \"@-@\", \"hodgkin\", \"lymphoma\", \".\"]}]], columns = ['sentence', 'd'])\n",
    "df = pd.concat([df, pd.json_normalize(df.d)],axis=1)\n",
    "df['tokens'] = df['toks'].apply(lambda x: [y[0] for y in x])\n",
    "df['sent'] = df['sentence'].apply(lambda x: x.replace(\"_\", \" \").replace(\"~\",\" \"))\n",
    "\n",
    "df['tokens_to_map'] = df.apply(lambda x: list(map(str, list(pipeline(x[\"sent\"])))), axis=1)\n",
    "df['token_map'] = df.apply(lambda x: tokenizations.get_alignments(x['tokens'],\n",
    "                                                              strip_g(x['tokens_to_map']))[1], axis=1)\n",
    "df['token_map_dict'] = df['token_map'].apply(lambda x: list_to_index_dict(x))\n",
    "df['weak_mwe'] = df.apply(lambda x: map_mwes_together(x, \"_\"), axis=1)\n",
    "df['strong_mwe'] = df.apply(lambda x: map_mwes_together(x, \"~\"), axis=1)\n",
    "\n",
    "display(df)\n",
    "df['token_map_dict'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c16902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "sent = \"His career was halted in 1997 when he was diagnosed with Non @-@ Hodgkin_lymphoma .\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "abc = tokenizer.tokenize(sent, padding=False,  truncation=True, max_length=50, return_tensors ='pt')\n",
    "print(len(abc))\n",
    "print(abc)\n",
    "abc = tokenizer(sent, padding=False,  truncation=True, max_length=50, return_tensors ='pt').input_ids[0]\n",
    "print(abc)\n",
    "print(tokenizer.decode(abc))\n",
    "print(len(abc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "abc = tokenizer.tokenize(sent)\n",
    "print(abc)\n",
    "print(len(abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4674c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "['his', 'career', 'was', 'halted', 'in', '1997', 'when', 'he', 'was', 'diagnosed', 'with', 'non', '@', '-', '@', 'ho', '##d', '##g', '##kin', '_', 'l', '##ym', '##ph', '##oma', '.']\n",
      "tensor([  101,  2010,  2476,  2001, 12705,  1999,  2722,  2043,  2002,  2001,\n",
      "        11441,  2007,  2512,  1030,  1011,  1030,  7570,  2094,  2290,  4939,\n",
      "         1035,  1048, 24335,  8458,  9626,  1012,   102])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.protobuf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_593088/3023841819.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3505\u001b[0m         \"\"\"\n\u001b[1;32m   3506\u001b[0m         \u001b[0;31m# Convert inputs to python lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3507\u001b[0;31m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m         return self._decode(\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_tf_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_torch_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mis_tf_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mTests\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSafe\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcall\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m_is_tensorflow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/core/framework/function_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.protobuf'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0329f5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
