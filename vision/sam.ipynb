{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24dd6f47-957b-40fd-ae82-484dea78f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f5e42b-e34e-49a4-aa31-0a5724b95c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar100 (/Users/andrejerkelens/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('cifar100', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edf93d2-ba8f-47d1-9199-71d95ecbac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31ef881-721a-4a2a-87ae-7c3c71254c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = sam_model_registry[\"default\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "predictor = SamPredictor(sam)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0406a8-1b25-4688-a298-d7cd2c251517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = dataset[1000]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508a4931-9efe-493c-8337-039af6c85039",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label = dataset[1000]['fine_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "487bdb1a-2cb8-432e-bd19-8318430e46ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b80305ad-813e-4ec5-acd6-2b56e79bfc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_image = np.array(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd3b270f-a3d4-4803-9d94-7e5d27e02edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUF0lEQVR4nO2by29cyXXGf3Xqce/tbjYpiZLHsceOgWQXZBEkmwBB/qjss8jfmWRljGF7LImP7r6PqjpZVNXtJjWWZpgACRAVwOGITd6u+uo8vvOd00ZV+brOS/63N/B/bX0F5Nn6Csiz9RWQZ+srIM+W+9yLv/71txpjZJomxnFknicUMICzwuu98M2tZd9BZ5UlKWoyKhGxHrGB61e/IvR7+v03gEe1AywGAZSW5DabDdvNFucdzjm22y3WCmINADlnfve733E4HPjjH/9YdmEEVaVlShHBOUsIHufKc1SVw+HAw+ORu/tH/uZv/57b25/xb//6L+YnA/LJMoYVEWPq94wIiIBVRU15qniHuA7f73BhS1aLZkhpQTUBBmcFEcH7gHcO6yzGmHrITM71fQBVxVqLtbYcFIMxQs4ZVV2/xxiB8n0YBp7SivOz/tz6IiCmbqg8y2CMoUFrRBGniANrq/9ZxfSC7QZsd83u5hdYu+PxEIkxMs/juuntdkPX9Wy3W4ZhoOv6+hrEmDAmYRJYawHw3tN1HV3Xlw0ZQ0qJnDMxludP00TOaQXAWouIYIyUs3yBdn0WEJFyAwBoxUQbypBTJsaMZlABI4JYi/UdNlxhww0pexQLqhijOOew1Q2urvb0FRDnPCLFYsrhHSkn5nnC2mI51lq894QQVkDKXnQFxDnLNI0sy8I8z1hbgNB2ji+szwKyWkdbCqvZZSVnJUWlvZcRELE41+PcDuuuUHWkJKhaxIBxClhEDNvtlq4b6Pthfa92k9Y6subVAowxKyjOO6gu0wBsgIgYUorrv3Muf5O1bfJ/ymXOiLSnkpIhzpCSIatgQ8D6LV34OT68xYXXYIdiIcHgXCB0G5zzOOvY7l7hrKsm/fS91hBV3SKlVC0hYZrriuCcuwATVDPDsMFaRwnaSkoJa4XNMCA1EL8IkE8sZIVDsQZs27Sa4krGYmzAuh3WbrB2QI2QMSUDeEffB7zzWOfxLhQft7rGpXLjJai2PeScV1CKCysgq9W032sWU1zLrYG2+Ht1Hc0scXk5IM9BMSSsGHa9YdspGw8WgezQHDCypdu+Q9weY6/IaQYy1hr6ruN6f4OIxRjBuVCACoLmRM4Jm8qtLvOEquKsZcyZeZ5rwMzFouQM1uVejZFqHecLFRFSVlJWxunA/D8GiCmZxAkFkN4wdCCioODdjhD2bLbXRO1JKqSoQKYLXc0W5ZkihWOIFB83TjB4Dg93xGUh1dtNypPUWqwlE4xgnFljSIszzULKvnU9h6JkzczzBOa/AciznyBAsHC9FXaDcrVJ5KUQrBCuGYbX7Pa3HKfMacqknDFkfAg4V4JhAQWsNZV8CcE7nLXcffie0+lYDqmQFFJK66GL62Ssc3Q1KzXu0gBp/84XmSVrApRpPpE+k3G+TMzM0+8l2CmWjJWMdwo+IHbL9ZtfMmzfAAHNMzkvOF8C336/owsdXd/jrMWIIcZEjEpG6bvAMPR0XUeKkZwy87IwjtPKRpvFNmCWZaHve0SEEAIxxjWItt8HQBUxBmctS0qfpSI/iqnqk0fUjaGFnVowLuD8QL+5xndX5GzJqmjOOCt47+j7jhACIXisLWa+LAuqoJVHFbpdaPeSF7IqcVkw9dZXXmTO1nIJVCNhl3Gl7JizmxqD/plk8UVAYoykmNCsa8ZNGWIGRdbM0e3f4Ydb3PAGlZ5xnJjGiWUeuXm9Yxh6NpseEYuIopqqScdCwbNhnmeOxyPb7QZnhd/+9reM08Q0zwTvsc7hvcdahzGCVkBTSiuhExG6rsSqy1Sdc0asYDTTBUfO9mWAzPNcyU1eb6L5dcxKTELKFuO22LBHbakxUoqoJoxRrKtEylRb00LozlxAV6Y5zzMpZbS+n2q95YtsASXLZD0XhpfraVA9B2S02LnIufT4yYDc39+v/ptVUWNQMUTgNGfCbDnOno27RTbfkFyApCzpBCxYq4Qu4IMvG6OYcvP1FmBVM9M0k2KENBOX5cnhSpqVwnRNPXAue1rdov5/KwDbhZ6tpIBiTSkzXgRIQ7fVAQaDmhJTxpgZsiPJFnE7nNvW342ktNQyvsNZh8GQYjyn8XpjwHoIzZllSRjNKND1PUZmUtY1haoWC/2hVdiovSB2YGtKLhbD+owXM9VmHettGi2gAGNUFrVk2SJui3MbNGdyTmRNONcxDEPJKEbI1ddXCUELsGLKxmNOpBSRGsJLAVeq3sZJVjcxZS/o0z1eZhaDIjV4O2tJKKpCzmDMCwERUw5fkkDZgKnoLgr4Df3VO0IYcMayxBGTM9Z5QtfT15pCSloodNwUSwNKejWJlM/xIOYMCtYFghq0y8xxIaVcgnlWsiaynt3EGENsFgjVUgwShWwTaAYyqRZ6n1s/ipitxGd9RVEjiPX4UA9tDEYVY1jVKuccVoSVM643XJ+y3uxS6byp3qQIYKQQN5ulZDrTNAhTQG2ptKbly32X+FNMUeR5zfPCtOvcuUBaA5PRUodYR9dtuLq6xvuAVD3Ei6XrQhV8OrwvMt54WFaxrfEEVSXlzDhPdF23ugmqxLiUIGgtNucCZI5FV4HKTYS+73HOrelXVddyoNF5bRdycckvAqSlr0adjWlmCs4Ggu/pajkv4uptmVXms84SQgeamY9net1K9mmeC8jwNAaIlEilGZCVv4gRsgFjzvpIs4ZSFrDqHk+I3MVq+38RIN77KrpIRbtt2uL9QN9v2G6u6EKPdY6Y7Lq5xjo3mw1o5nB/tx647/tKnjI561qgPalHoABiEjZXDpEUCyuxuiRk3tfU/gyQdqnlqwH/QkDag5/+G5yz3Ny8Yn/1is1mj5iSNtHy+nrAnOttFB7RpLVmyu32nHPr37T33G4GrBisMYX5LgtJD8xLJKVzMG1c41zUscahSysqgJfnv7iWuQxEQA2OBhHLZrNlGDZ0oSfGomUYUzLTp0GuECo1ZbdrDKlCbeMJWqgpRkps8M4SnEOsRcaJw3EiJV2fbwwrk27x55KoPTkL5+TwufV5QAC5EN3VGJxYuq7j9u079tevCKEjpxM5K33X4bwj1KDa9/2qcpUgl9ZyoGinUsr/WvmmuJTg6h23b96w2Qzsdzvev//A/cMDd49HYk4EwrrHaZxQjUh9VhGVP71YMQZbb+fF1N00gYUS2QVFqsjb9T3OO9SUjBCXGRfKzzabDdvtjmEYiMtCjAvO+aJDmKWq0SW4UsuBEjMU7x1dCOx2O4a+p+97ur4jzBPWWUQsZ6MvsUU114KRNX0/B6RZ1OdzzBdjiGI0YykthMIgPX3fsd1v8b0j5YXj4Z5pnNhcDWyGDd9883P2+z273Y6PHz4wjmPJVEdhXmKVAIoliRiMNczTxDzNXG237LY7vnn3Ducsqlp6MRWcGDPTHFFYyVnRQCJWDO4ZL7ks+9ulvhgQ8wP+5p0jBI93hriMfPz4Pff3H0hLxNlf4Gsbses6+r6vLUnLaRxRhSVG+r4nhMD+ek8Ins124PHhgceHR4Jz9F2/pvvmYpoV7zze+1p166dxwhics0+JX/teKGztpLyQmNWnnh+uWmKEdzhbALk7PvBwf4dmal+1SIWtyxY3G0SE4XhEq1i83W7p+563b9+y2Qy8en3Dh/fvef/+PaSMs8UtYoycTieWGMmqOOtWN6Ox3GeAWGtpurxe7FsBRFCRKoS+EJDnlLfzAS+W08MHxuOBu/fvsRLohy1dXxSxlg4fHx8Zx5FxHHm4v2ee5zXAdl0o8aHrEGtLLMm6bnUcx1VU1vrV9uG9Q1IumStFcq0NREzhI6pkrYpcfYZB8DaQE/CZJt6PGod4zgpBWcYT4+GBw8MHlvmE5vhE5G3mvixLDazxiTIuYtcU3JS5ko7Pws5zlonhyXsYqV/PFPa1j1vXmfBZJJtWrf7g+lGaagNDVckpEqeR+z/9nun0yHL6SPCOnHtSOh96miamsTS2l2VZKbtzjhBKg+rj3R3mwWA/GMbTyDSObIdNAeuCaVqxtaElGDHklJ/UJmKKyHQpdbb9ti8xghVhTgssfx6RL7vMBSAAcVmYJjgdlLicMBrJaSYuM/M8rcyxraKUsRZebbNQlTMUlkyKqbhDjT/tzpuEaAArttQzVWCOuYBwWXymlEqs4Gl9ZAQQQ9REzC/sy9TdI3VTagzTaULjzJ0ccCZhJRKXkWk6cjw8stlescxLYZ9i1pu01j4RcHIuLcWcE0uccdbhXeE3oQGiWih/fX/rHGKlWF2MLHEpMkHl6ylnlhjXmNQq9SZBIjDpwpSnlwEiRfIkm7Mc50SwhqKMSWlCaZyJ08jDx4/03Yb53YR1DmsdqU4JNPNvbtOkRIMh+BJg+yoBWGtJOaFqiCkTcyamWvdYy3Y7cDweiUsZdxAjOOsJwQOf0vYyYMOq1I/TSwGRIvfJhSgjppTmJZWVt9GUSMvM6XhgPB2J80yNgGV2RFsj2lYeIWsR1rJGFwooYgWqypjbV1aSnsem+s6zzBZryt5EDK62Ktaey4W7lN5unTBKJaa9DJCqTViRtVIUo1ijSKPzpliO0czD3Uc2mx3TNGKsxXmP6zyqyimVGDOOx1UZ994RQsebN6/XrPT4+AiUmTPVi3YhrZI2hNCx3WTEwOFwZFmWIhnKp5mlgbgsC6dxZJnOqv5PBuR5+Vx+1sYLzt12qpK9LEtpHuVY9dPaD9FMjAspRVKMOFe0C++rqJMzp9OJ0+nENE1rGV+yjaxxoAHZDn85OnEelTjv/TKQp5xrpzD/IAP/UYBAkzBNnaRSMBnkUpyBnEtUn5aJcR5ZlomchyoFzsS4cH9/V8mVVjbr2W43pBT5/vvv+f777/nTn/60ahu3t7eE0NEPW0IIdF0HlODcrOgyVhQrkSdu0kSjaZqYp4nD4yPkhLcvpO63r9/gvGfYbqhdIiTNGBJOFjQv5DgzzxFVWW+vWYcR5f7ugWkaOZ2OdF1gf3VVBCFrWZaJpUin9fZ09e/7+we8HzmeJkIIaylQYo5nWlW8cwO8tS3P/Znz/NmyzCxzcZec4ssA+dnbd2x2W7755V9grMVYQ5xO5DizTAem05HT4YHD4UhMGRcc1kkVioq/3z/ccTwemKeJvg9cX+/X/szd3V0hWcraRogxVh7zUJilPayAvHv3DoDg/YWgLGsvtwFSahpZg/ayLCzzwjxPxGUmxxcC8qtvv+Uvvv0l//jP/0S3GfBdYJ5OxLgwn07M08R0OnJ395F5msA6ttsrfvHLX9FvtoSuJ6bE8XREjLK/2vP27e06JxKCJ2elCz3H45HD4cDvf/97TqcTMSa892y2u7Vx/Yc//GEVlYwxDMPANE2lPIgJ5y5GqCit2MPhwHfffcd4OnI6HtFmki8B5NXrV7x9946//M1vGHZbQt8xzxMpReZTQXueJu7vPjJNIzEluq7n1etbfOiwzvP69SuGccBZYbfdcH19vSrkbRyi74ZVCDqdTjVOHHDe0/f9eujTaawidYknhc8Irf5ZWyUUy8gpkWJkqezZtL/5zJk/P9r913/Fu29+hguhkKfQ0YUORdGdlkaSmLVbb0Rqe7GV34ZXr9+UmKJ5nTNtN/zzn/+i0uta+0wjb25vub+/5z/+/T9x1rMZdqShMNPvvvuusNvUXMWvE4en04iIVJFpQx8C727fsN9tyxC5loB+c72nqwTuJwPy7a9/xfXNDT74UlhdKthGV0EZqbdiLdQRqAZKyQ6lx/K0LQAhNJXc1NLdscRI13WcjmNhuFjGcQSoabqod2Ucy7Lf70gps9stZUrIWV6/vmF/tcNaS4yR/X5XZUZlt92s6f4nA/J3//D3F7daeyHmTJYuB++h9pWgaKacAWtzPJdCVQl4VRA2UodxoR8GYozc3NxwOBz5+P4jHz58IGtks+mZZ2FZZrwXui6w270hBP/kfd68vmF/teXm5qaw1JhItZne5lReBMgP9Uuf/+zit2ltw09fO6tuZ0DOAzGtBdEUL2Pg6mpH8I7ghX5wHA5btruOuCzMcSG4MuK9218RfKl/Suml9J0nBEdRERTfCTZDSmX47sWNqksXec78Pjn02rs5ayfPQWjrzCiLyzyB1ZSx7mHo8U7w3mCdYdgEhsHXSnepMkFgd3WF976OXVS5ghKzlqV8yMA5u45BpNYnfgkgbfNt1MnaiPfuTNnRZ5bw6Sqvm9qWLACdRxKq1mHOJOryu3WWQQYw0HWhpum8kq/2tboetZeERagFZNZaYD7VYl4EyKqS5YwxZc7TWllriCeHbnMO5uJndf3Zbpn5rABerK0Wl9bKWgMVUGUdBG7vYc5/WIb5kFJP0ToICgjmpTGkuUhTqNrNFIZ49vn1wF9oBF0G4QLi+QBPgDAG1doeU0hJiUuu7lU6fWIEo2ZVxsqHAkr1nVY3NIixWFc+aqJxQVNcZ91eAEjZWPmoVtt7DV31BzGlOrzyVCZs/10bh3VILtcGuMEg1tbAetZIz3JgqjppUde9C1hx6++1IZhVjFaqOl97x3U2TeunvUoGLADpS9sQrUAqfdlCLMxFpsm5iERn3bU2sXO+2Li0k543W190zfcbkVuBaB8ZaxKgw4Y2Fn5ukgPk9nkahUQipkiKmZzyWo1bW0cgtKhr5qVTiM5JEWnUrPyjETSpolEZjSzbFCm9lVTLfKUVySUWrN3iC2szXGQvY8rHw7QCqefAbS6CL6YO4KGk+hzX9aQYmbREiSQZ55pK5+qdtMmBFwLSDl0EoHKgc3+mtAXNysa0KuUVjIuM0TJNc5UfCjRrqr4ES542yZ7QADIpp7oXgyCQFSsWrcy5yQCX/Ok5l/pkH1+al/j/tr5+kPnZ+grIs/UVkGfrKyDP1ldAnq2vgDxb/wVNgj/WaHNHLgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(cv_image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bcd5535-6d0d-40f6-adbb-b506f1df86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(cv_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62b3757b-15a8-47c5-ac1c-3663a7fb239e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(cv_image)\n",
    "show_img(masks)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61570ea-f1f6-47b5-b408-43be7e172760",
   "metadata": {},
   "source": [
    "<b> Labels are currently wrong for tiny imagenet, though the predictions are correct (since they are in training set), I cannot currently find a mapping from tiny imagenet to regular imagenet so will have to assume the pred is correct </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23cf9e-9b14-4d17-b081-d97618d6d4ff",
   "metadata": {},
   "source": [
    "<b> Q1: should i be using the OHE label or the logits? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "059773d3-cebd-4461-bab7-801b9045f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3cac985-3a81-4b4d-826e-75278956bad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbit\n"
     ]
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained(\"Ahmed9275/Vit-Cifar100\")\n",
    "classifier = AutoModelForImageClassification.from_pretrained(\"Ahmed9275/Vit-Cifar100\")\n",
    "\n",
    "inputs = processor(cv_image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = classifier(**inputs).logits\n",
    "\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_label = logits.argmax(-1).item()\n",
    "print(classifier.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23a0eeb7-6ce2-4a0b-98e6-f1151d417e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35e7735-94b0-419a-80c8-6a053850c13d",
   "metadata": {},
   "source": [
    "### Calculate Shapley Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23afa877-8665-4abd-b010-3dab07bee20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channel = cv_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cf8594-3bf2-4377-89f0-ba784eb8d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits\n",
    "# logits.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68998312-e304-47bb-a8f4-a88001eb9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for channel in range(channel):\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            for k in range(i, width):\n",
    "                for l in range(height):\n",
    "                    if k == i and l <= j:\n",
    "                        continue\n",
    "                    pairs.append(((j,i,channel), (j,i,channel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3db13139-e882-4394-bcc6-47661bb6abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1571328\n",
      "4717056\n"
     ]
    }
   ],
   "source": [
    "# If we go pairwise within each channel\n",
    "print(math.comb(32*32,2)*3)\n",
    "# if we go pairwise across all channels\n",
    "print(math.comb(3*32*32,2))\n",
    "# We will modify on the raw image dim to reduce number of pairs, vs altering the processed inputs via VIT pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b862a573-395b-4002-8a14-ab22f6dd95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(pairs) == math.comb(height * width, 2) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac9a4038-3530-45f9-8d95-9e7e612f9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_value = 0 # Can change this to noise/avg/ some other reference value\n",
    "cuda = False\n",
    "\n",
    "def get_shapval(p1, p2, img, instruction:str):\n",
    "    img1 = copy.deepcopy(img)\n",
    "\n",
    "    if instruction == \"a+b\":\n",
    "        pass\n",
    "    elif instruction == \"a\":\n",
    "        img1[p1] = reference_value\n",
    "    elif instruction == \"b\":\n",
    "        img1[p2] = reference_value\n",
    "    elif instruction == \"phi\":\n",
    "        img1[p1] = reference_value\n",
    "        img1[p2] = reference_value\n",
    "\n",
    "    inputs = processor(img1, return_tensors=\"pt\")\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        logits = classifier(**inputs).logits\n",
    "\n",
    "    return logits.softmax(dim=-1)\n",
    "\n",
    "\n",
    "def get_interaction(pixel_pair, sample_image):\n",
    "\n",
    "    p1, p2 = pixel_pair\n",
    "    np_img = np.array(sample_image)\n",
    "\n",
    "    phi = get_shapval(p1, p2, np_img, \"phi\")\n",
    "    a = get_shapval(p1, p2, np_img, \"a\")\n",
    "    b = get_shapval(p1, p2, np_img, \"b\")\n",
    "    apb = get_shapval(p1, p2, np_img, \"a+b\")\n",
    "\n",
    "    val = apb - a - b + phi\n",
    "    \n",
    "    if cuda:\n",
    "        val = torch.linalg.norm(val, dim=-1).cpu()\n",
    "    else:\n",
    "        val = np.linalg.norm(val, axis=-1)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7369c15d-e393-4ba5-a7a3-3a6237e86526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1571328"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31540ef8-b11e-4839-b4f4-ba128b12ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: 0\n"
     ]
    }
   ],
   "source": [
    "interactions = []\n",
    "num_pairs = len(pairs)\n",
    "\n",
    "for idx, pair in enumerate(pairs):\n",
    "    if idx%100000==0:\n",
    "        print(f\"Complete: {idx}\")\n",
    "    interactions.append(get_interaction(pair, sample_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e220d82-92a6-426f-9fd5-c00cde2e9bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b8cc356-24de-4f00-aae6-d3a01b255489",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Calculate shapley residuals\n",
    "\n",
    "### Ignore\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3dfd08e-76e1-42c8-9dd8-378a5e86e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88a40dd2-0ce3-4e54-b217-f6324e61a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv_image.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "185382e6-0148-4d50-8352-0803b87466ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_c, _w, _h = torch.squeeze(inputs['pixel_values']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2d76f83-80a6-443d-905b-2632f3afe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_dim = _c * _w * _h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe66b4-5113-45bd-b890-2eb00c459211",
   "metadata": {},
   "source": [
    "### Calculate shapley residual for each pixel? or calculate shapley residual for each patch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c812b556-d876-4fc1-a0da-c551b2b0804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coalitions(N):\n",
    "    # Does not account for empty coalition\n",
    "    coalitions = []\n",
    "    for i in range(1, 2 ** N):\n",
    "        coalition = [j for j in range(N) if (i & (1 << j)) > 0]\n",
    "        coalitions.append(tuple(coalition))\n",
    "    return coalitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "37ef692d-e34a-4a57-848e-e784e595ffda",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "value_function() missing 1 required positional argument: 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m shapley_residuals\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Example calculation for one image\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m shapley_res \u001b[38;5;241m=\u001b[39m shapley_residual(cv_image)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShapley Residuals for the image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshapley_res\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[182], line 36\u001b[0m, in \u001b[0;36mshapley_residual\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     35\u001b[0m     v_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m  \u001b[38;5;66;03m# set initial value?\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     d_i \u001b[38;5;241m=\u001b[39m d_i_v(i, img)\n\u001b[1;32m     37\u001b[0m     d_v \u001b[38;5;241m=\u001b[39m d_v_i(v_i, i, img)\n\u001b[1;32m     38\u001b[0m     shapley_residuals[i] \u001b[38;5;241m=\u001b[39m d_i \u001b[38;5;241m-\u001b[39m d_v\n",
      "Cell \u001b[0;32mIn[182], line 23\u001b[0m, in \u001b[0;36md_i_v\u001b[0;34m(i, img)\u001b[0m\n\u001b[1;32m     21\u001b[0m S_with \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m-\u001b[39m {i}\n\u001b[1;32m     22\u001b[0m S_without \u001b[38;5;241m=\u001b[39m S_with\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value_function(S_with, img) \u001b[38;5;241m-\u001b[39m value_function(S_without, img)\n",
      "\u001b[0;31mTypeError\u001b[0m: value_function() missing 1 required positional argument: 'label'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from scipy.optimize import minimize\n",
    "# from itertools import combinations\n",
    "# import math\n",
    "\n",
    "# # Define value function v(S)\n",
    "# def value_function(S, img, label):\n",
    "#     temp_img = np.copy(img)\n",
    "#     # Zero out the pixels not in S\n",
    "#     # TODO: Decide if to 0 or add noises to pixels, may lead to different results, per https://arxiv.org/pdf/1704.02685.pdf\n",
    "#     temp_img[0, list(S)] = 0  \n",
    "#     inputs = processor(temp_img, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         logits = classifier(**inputs).logits\n",
    "#     # prediction = logits.argmax(-1).item()\n",
    "#     return CrossEntropyLoss()(logits, torch.tensor([label])).detach().numpy()\n",
    "\n",
    "# def d_i_v(i, img):\n",
    "#     S_with = set(range(img.shape[1])) - {i}\n",
    "#     S_without = S_with\n",
    "#     return value_function(S_with, img) - value_function(S_without, img)\n",
    "\n",
    "# def d_v_i(v_i, i, img):\n",
    "#     objective = lambda v: np.linalg.norm(d_i_v(i, img) - v)\n",
    "#     res = minimize(objective, v_i, method=\"SLSQP\")\n",
    "#     return res.x[0]\n",
    "\n",
    "# # Compute Shapley residual for an image\n",
    "# def shapley_residual(img):\n",
    "#     N = img.shape[1]\n",
    "#     C,W,H  = torch.squeeze(processor(img, return_tensors=\"pt\")[\"pixel_values\"]).shape\n",
    "    \n",
    "#     shapley_residuals = np.zeros(C,W,H)\n",
    "#     for c in range(C):\n",
    "#         for w in range(W):\n",
    "#             for h in range(H):\n",
    "#                 v_i = 10  # set initial value?\n",
    "#                 d_i = d_i_v(i, img)\n",
    "#                 d_v = d_v_i(v_i, i, img)\n",
    "#                 shapley_residuals[c][w][h] = d_i - d_v\n",
    "#     return shapley_residuals\n",
    "\n",
    "# # Example calculation for one image\n",
    "# shapley_res = shapley_residual(cv_image)\n",
    "# print(f\"Shapley Residuals for the image: {shapley_res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a09fb-3b99-4ffa-a582-5b7bcab29416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ba72741-1661-4f64-8d44-3a234d9182c4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc298625-1623-45be-a752-7fbfd7ef8333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f5c711-1834-47de-9fca-692c78cbc581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shapleyres",
   "language": "python",
   "name": "shapleyres"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
