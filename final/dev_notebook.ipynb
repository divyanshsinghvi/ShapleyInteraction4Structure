{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af294e1d-0794-4a9a-b981-69ad7c9d4001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-21T04:54:44.959849Z",
     "iopub.status.busy": "2023-08-21T04:54:44.959646Z",
     "iopub.status.idle": "2023-08-21T04:54:46.773483Z",
     "shell.execute_reply": "2023-08-21T04:54:46.772607Z",
     "shell.execute_reply.started": "2023-08-21T04:54:44.959830Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dependencies..\n",
      "Seq Length: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing dependencies..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.linalg import norm \n",
    "import pandas as pd\n",
    "import os\n",
    "# from util import RegressionGame\n",
    "# from util_sparse import getShapleyProjection\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# import shap\n",
    "# import llm_helper\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "if not os.path.exists('__dcache__'):\n",
    "    os.makedirs('__dcache__')\n",
    "\n",
    "# N = 6000\n",
    "# print(\"Explanation count: %s\" % N)\n",
    "# k = 100\n",
    "# print(\"SHAP sample count: %s\" % k)\n",
    "seq_len = 50\n",
    "print(\"Seq Length: %s\" % seq_len)\n",
    "\n",
    "# Setup\n",
    "np.random.seed(1)\n",
    "# model = llm_helper.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da144645-241a-4cde-991e-f73e335410db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d149b369-2ff7-4d2f-a42a-4cde9979fee4",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe7eff-15a4-47fb-8f63-82b7bde0ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "if cuda:\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased').cuda()\n",
    "else:\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "test['length'] = test['sentence'].str.split().str.len()\n",
    "\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X, token_next):\n",
    "    if cuda:\n",
    "        return torch.softmax(model(X).logits[token_next:], dim=1)\n",
    "    else:\n",
    "        return scipy.special.softmax(model(X).logits[token_next:].detach().numpy(), axis=1)\n",
    "\n",
    "def interaction_value_di(model, X, tokens):\n",
    "    token1, token2 = tokens\n",
    "    \n",
    "    token_next = max(token1, token2) + 1\n",
    "    AB = get_prediction_softmax(model, X, token_next)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1, token_next)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[token2] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2, token_next)\n",
    "\n",
    "    X_t12 = X.clone()\n",
    "    X_t12[token2] = tokenizer.unk_token_id\n",
    "    X_t12[token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12, token_next)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    if cuda:\n",
    "        val = torch.linalg.norm(val, dim=1).cpu()\n",
    "    else:\n",
    "        val = np.linalg.norm(val, axis=1)\n",
    "    \n",
    "    del X_t1, X_t2, X_t12,  AB, A, B, phi\n",
    "    if cuda:\n",
    "        return val.detach(), token_next\n",
    "    else:\n",
    "        return val, token_next\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col, row_number):\n",
    "    iv_mwe = []\n",
    "    # col = 'weak_mwe' | 'strong_mwe'\n",
    "    \n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "                    \n",
    "                    iv = interaction_value_di(model, encoded_row, [mwe[i], mwe[j]])\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-mwe[j])),mwe, row_number, i, j])\n",
    "    return iv_mwe\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows()):\n",
    "    # if row_number%100==0:\n",
    "        # print(row_number, len(test), f'{row_number*100/len(test)}%')\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "    \n",
    "    weak_mwe_distance.extend(mwe_distance_interaction(abc, row, 'weak_mwe', row_number))\n",
    "    strong_mwe_distance.extend(mwe_distance_interaction(abc, row, 'strong_mwe', row_number))\n",
    "    \n",
    "    del abc\n",
    "\n",
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('weak_mwe_distance_3d_mlm.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('strong_mwe_distance_3d_mlm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f03b9b-4541-429a-89ae-469c2a2a4c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"Fetching Average Distance...\")\n",
    "cuda = True\n",
    "\n",
    "\n",
    "def calculate_interaction(encoded_row, row, row_number):\n",
    "    interactions = []\n",
    "    encoded_len = len(encoded_row)\n",
    "    for j in range( min(seq_len, encoded_len)):\n",
    "        for k in range(j+1, min(seq_len, encoded_len, j+8)):\n",
    "            if j+k >= encoded_len:\n",
    "                continue\n",
    "            if encoded_row[j] == tokenizer.unk_token_id or encoded_row[k] == tokenizer.unk_token_id:\n",
    "                continue\n",
    "            \n",
    "            iv = interaction_value_di(model, encoded_row, [j, k])\n",
    "            interactions.append([iv, abs(k -j),row_number, [j,k]])\n",
    "    return interactions\n",
    "\n",
    "average_distance = []\n",
    "start_time =  datetime.datetime.now()\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows()):\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    \n",
    "    average_distance.extend(calculate_interaction(abc, row, row_number))\n",
    "\n",
    "\n",
    "pd.DataFrame(average_distance, columns = ['I', 'posdis', 'row_number', 'word_pair']).to_pickle('average_distance_3d_mlm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584b85a-0159-4ff7-b5a7-5419997adce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e483ca-93dc-4903-912c-e9db286c64c2",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77f68b9-de64-46ad-b985-22bf0bd9dec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-21T04:54:46.775635Z",
     "iopub.status.busy": "2023-08-21T04:54:46.775143Z",
     "iopub.status.idle": "2023-08-21T04:54:52.040180Z",
     "shell.execute_reply": "2023-08-21T04:54:52.039382Z",
     "shell.execute_reply.started": "2023-08-21T04:54:46.775610Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "if cuda:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "test['length'] = test['sentence'].str.split().str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985ed49-22ea-4718-b24d-7b69959a7fc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T17:55:58.899122Z",
     "iopub.status.busy": "2023-08-22T17:55:58.898538Z",
     "iopub.status.idle": "2023-08-22T17:56:02.051164Z",
     "shell.execute_reply": "2023-08-22T17:56:02.049902Z",
     "shell.execute_reply.started": "2023-08-22T17:55:58.899094Z"
    }
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X, token_next):\n",
    "    if cuda:\n",
    "        return torch.softmax(model(X).logits[token_next:], dim=1)\n",
    "    else:\n",
    "        return scipy.special.softmax(model(X).logits[token_next:].detach().numpy(), axis=1)\n",
    "\n",
    "def interaction_value_di(model, X, tokens):\n",
    "    token1, token2 = tokens\n",
    "    \n",
    "    token_next = max(token1, token2) + 1\n",
    "    AB = get_prediction_softmax(model, X, token_next)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1, token_next)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[token2] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2, token_next)\n",
    "\n",
    "    X_t12 = X.clone()\n",
    "    X_t12[token2] = tokenizer.unk_token_id\n",
    "    X_t12[token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12, token_next)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    if cuda:\n",
    "        val = torch.linalg.norm(val, dim=1).cpu()\n",
    "    else:\n",
    "        val = np.linalg.norm(val, axis=1)\n",
    "    \n",
    "    del X_t1, X_t2, X_t12,  AB, A, B, phi\n",
    "    if cuda:\n",
    "        return val.detach(), token_next\n",
    "    else:\n",
    "        return val, token_next\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col, row_number):\n",
    "    iv_mwe = []\n",
    "    # col = 'weak_mwe' | 'strong_mwe'\n",
    "    \n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "                    \n",
    "                    iv = interaction_value_di(model, encoded_row, [mwe[i], mwe[j]])\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-mwe[j])),mwe, row_number, i, j])\n",
    "    # print(torch.cuda.memory_allocated()/(1024*1024*1024))\n",
    "    return iv_mwe\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows()):\n",
    "    # if row_number%100==0:\n",
    "        # print(row_number, len(test), f'{row_number*100/len(test)}%')\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    \n",
    "    # print(\"Start\",torch.cuda.memory_allocated()/(1024*1024*1024))\n",
    "    weak_mwe_distance.extend(mwe_distance_interaction(abc, row, 'weak_mwe', row_number))\n",
    "    strong_mwe_distance.extend(mwe_distance_interaction(abc, row, 'strong_mwe', row_number))\n",
    "    \n",
    "    # print(torch.cuda.memory_allocated()/(1024*1024*1024))\n",
    "    del abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5accf9-9ca1-4196-a14f-d02b4a02b1d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T17:55:58.899122Z",
     "iopub.status.busy": "2023-08-22T17:55:58.898538Z",
     "iopub.status.idle": "2023-08-22T17:56:02.051164Z",
     "shell.execute_reply": "2023-08-22T17:56:02.049902Z",
     "shell.execute_reply.started": "2023-08-22T17:55:58.899094Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b60c9-bbb0-4e29-8260-1fa98aeb85f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3e8a36-7551-4e6a-9612-bb19fd8f3b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-21T05:18:52.103707Z",
     "iopub.status.busy": "2023-08-21T05:18:52.103384Z",
     "iopub.status.idle": "2023-08-21T05:18:52.122089Z",
     "shell.execute_reply": "2023-08-21T05:18:52.121225Z",
     "shell.execute_reply.started": "2023-08-21T05:18:52.103687Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.cuda.memory_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870cb30b-ef69-46af-8191-0d166e60f302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-21T05:18:52.123658Z",
     "iopub.status.busy": "2023-08-21T05:18:52.123056Z",
     "iopub.status.idle": "2023-08-21T05:18:53.491513Z",
     "shell.execute_reply": "2023-08-21T05:18:53.490727Z",
     "shell.execute_reply.started": "2023-08-21T05:18:52.123633Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('weak_mwe_distance_3d.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('strong_mwe_distance_3d.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6478f6ae-f304-4949-95fe-83144894a395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-21T05:18:53.494262Z",
     "iopub.status.busy": "2023-08-21T05:18:53.494030Z",
     "iopub.status.idle": "2023-08-21T05:18:53.642585Z",
     "shell.execute_reply": "2023-08-21T05:18:53.641786Z",
     "shell.execute_reply.started": "2023-08-21T05:18:53.494243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>posdis</th>\n",
       "      <th>ignore</th>\n",
       "      <th>row_number</th>\n",
       "      <th>first_token</th>\n",
       "      <th>second_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([tensor(0.2753)], 40)</td>\n",
       "      <td>1</td>\n",
       "      <td>[38, 39]</td>\n",
       "      <td>373</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([tensor(0.2149), tensor(0.0847), tensor(0.092...</td>\n",
       "      <td>1</td>\n",
       "      <td>[12, 13]</td>\n",
       "      <td>418</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([tensor(0.0558), tensor(0.2328), tensor(0.130...</td>\n",
       "      <td>1</td>\n",
       "      <td>[5, 6, 7]</td>\n",
       "      <td>462</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([tensor(0.1867), tensor(0.1216), tensor(0.149...</td>\n",
       "      <td>2</td>\n",
       "      <td>[5, 6, 7]</td>\n",
       "      <td>462</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([tensor(0.3418), tensor(0.1314), tensor(0.195...</td>\n",
       "      <td>1</td>\n",
       "      <td>[5, 6, 7]</td>\n",
       "      <td>462</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>([tensor(0.6572), tensor(0.2784), tensor(0.260...</td>\n",
       "      <td>1</td>\n",
       "      <td>[11, 12, 13, 14, 15]</td>\n",
       "      <td>10006</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>([tensor(0.3913), tensor(0.0865), tensor(0.028...</td>\n",
       "      <td>4</td>\n",
       "      <td>[11, 12, 13, 14, 15]</td>\n",
       "      <td>10006</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>([tensor(0.3599), tensor(0.1236), tensor(0.045...</td>\n",
       "      <td>3</td>\n",
       "      <td>[11, 12, 13, 14, 15]</td>\n",
       "      <td>10006</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>([tensor(0.3050), tensor(0.2664), tensor(0.046...</td>\n",
       "      <td>2</td>\n",
       "      <td>[11, 12, 13, 14, 15]</td>\n",
       "      <td>10006</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>([tensor(0.4466), tensor(0.2837), tensor(0.061...</td>\n",
       "      <td>1</td>\n",
       "      <td>[11, 12, 13, 14, 15]</td>\n",
       "      <td>10006</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     I  posdis  \\\n",
       "0                               ([tensor(0.2753)], 40)       1   \n",
       "1    ([tensor(0.2149), tensor(0.0847), tensor(0.092...       1   \n",
       "2    ([tensor(0.0558), tensor(0.2328), tensor(0.130...       1   \n",
       "3    ([tensor(0.1867), tensor(0.1216), tensor(0.149...       2   \n",
       "4    ([tensor(0.3418), tensor(0.1314), tensor(0.195...       1   \n",
       "..                                                 ...     ...   \n",
       "324  ([tensor(0.6572), tensor(0.2784), tensor(0.260...       1   \n",
       "325  ([tensor(0.3913), tensor(0.0865), tensor(0.028...       4   \n",
       "326  ([tensor(0.3599), tensor(0.1236), tensor(0.045...       3   \n",
       "327  ([tensor(0.3050), tensor(0.2664), tensor(0.046...       2   \n",
       "328  ([tensor(0.4466), tensor(0.2837), tensor(0.061...       1   \n",
       "\n",
       "                   ignore  row_number  first_token  second_token  \n",
       "0                [38, 39]         373            1             0  \n",
       "1                [12, 13]         418            1             0  \n",
       "2               [5, 6, 7]         462            1             0  \n",
       "3               [5, 6, 7]         462            2             0  \n",
       "4               [5, 6, 7]         462            2             1  \n",
       "..                    ...         ...          ...           ...  \n",
       "324  [11, 12, 13, 14, 15]       10006            3             2  \n",
       "325  [11, 12, 13, 14, 15]       10006            4             0  \n",
       "326  [11, 12, 13, 14, 15]       10006            4             1  \n",
       "327  [11, 12, 13, 14, 15]       10006            4             2  \n",
       "328  [11, 12, 13, 14, 15]       10006            4             3  \n",
       "\n",
       "[329 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63edc100-ea32-4e52-a4d3-9ab5f31803fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T18:04:16.914446Z",
     "iopub.status.busy": "2023-08-22T18:04:16.913801Z",
     "iopub.status.idle": "2023-08-22T18:05:50.412901Z",
     "shell.execute_reply": "2023-08-22T18:05:50.411339Z",
     "shell.execute_reply.started": "2023-08-22T18:04:16.914423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"Fetching Average Distance...\")\n",
    "cuda = True\n",
    "\n",
    "\n",
    "def calculate_interaction(encoded_row, row, row_number):\n",
    "    interactions = []\n",
    "    encoded_len = len(encoded_row)\n",
    "    for j in range( min(seq_len, encoded_len)):\n",
    "        for k in range(j+1, min(seq_len, encoded_len, j+8)):\n",
    "            if j+k >= encoded_len:\n",
    "                continue\n",
    "            if encoded_row[j] == tokenizer.unk_token_id or encoded_row[k] == tokenizer.unk_token_id:\n",
    "                continue\n",
    "            \n",
    "            iv = interaction_value_di(model, encoded_row, [j, k])\n",
    "            interactions.append([iv, abs(k -j),row_number, [j,k]])\n",
    "    return interactions\n",
    "\n",
    "average_distance = []\n",
    "start_time =  datetime.datetime.now()\n",
    "# min_row = average_distance[-1][2]\n",
    "# print(min_row)\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows()):\n",
    "    # if i%9 == 0 or i<=100:\n",
    "    # print(i, len(test), f'{i*100/len(test)}%', (datetime.datetime.now() - start_time).seconds)\n",
    "    # start_time = datetime.datetime.now()\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    \n",
    "    average_distance.extend(calculate_interaction(abc, row, row_number))\n",
    "    if row_number % 1000 == 0:\n",
    "        print(average_distance[-1][-1])\n",
    "\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# \n",
    "# num_cores = 9\n",
    "# for x in range(0, len(test), 99):\n",
    "#     print(x, len(test), f'{100*x/len(test)}%')\n",
    "#     fabc =  lambda x : tokenizer(x, padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    \n",
    "#     average_distance.extend(Parallel(n_jobs=num_cores)(\n",
    "#         delayed(calculate_interaction)(fabc(row['sentence']), row, i) for i, row in tqdm(test[x:x+99].iterrows())))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6829488-fe87-49e0-b606-a39640d7f393",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T18:05:50.414734Z",
     "iopub.status.idle": "2023-08-22T18:05:50.414986Z",
     "shell.execute_reply": "2023-08-22T18:05:50.414879Z",
     "shell.execute_reply.started": "2023-08-22T18:05:50.414867Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(average_distance, columns = ['I', 'posdis', 'row_number', 'word_pair']).to_pickle('average_distance_3d.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91405974-39af-486a-a013-6b2b4c313d57",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4b2769-b5ee-46fc-b063-8ff994687038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T08:37:22.827796Z",
     "iopub.status.busy": "2023-08-22T08:37:22.827583Z",
     "iopub.status.idle": "2023-08-22T08:37:23.177040Z",
     "shell.execute_reply": "2023-08-22T08:37:23.176272Z",
     "shell.execute_reply.started": "2023-08-22T08:37:22.827780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 9236, 2: 1729, 3: 328, 4: 71, 5: 18, 6: 7, 7: 2}\n",
      "(array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "        80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
      "        93,  97,  98,  99, 101, 102, 105, 106, 109, 110, 112, 113, 115,\n",
      "       116]), array([528, 979, 878, 762, 764, 763, 766, 794, 787, 722, 769, 770, 678,\n",
      "       696, 694, 669, 592, 577, 543, 474, 498, 466, 416, 422, 404, 335,\n",
      "       296, 295, 283, 259, 230, 192, 169, 144, 146, 147, 134, 117, 109,\n",
      "        86,  75,  78,  79,  74,  60,  53,  52,  42,  41,  40,  44,  37,\n",
      "        17,  19,  21,  16,  15,  21,  19,  15,  12,   9,   9,   7,   5,\n",
      "         6,   7,   7,   4,   1,   2,   2,   3,   1,   3,   2,   3,   1,\n",
      "         2,   2,   1,   1,   2,   3,   2,   3,   6,   5,   1,   1,   1,\n",
      "         1,   1,   2,   1,   1,   1,   1,   1,   2,   2,   1,   1,   1,\n",
      "         1]))\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "a = np.unique([y[-1]-y[0] for x in test['_'].to_list() for y in x], return_counts=True)\n",
    "print({x:y for x,y in zip(a[0], a[1][::-1].cumsum()[::-1])})\n",
    "print(np.unique([z for x in test['_'].to_list() for y in x for z in y], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbd21c-dcc2-4148-9f5c-a9e78d80e7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbcc8062-e2f8-4fbc-88c9-40a582d4041d",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7725b98f-ac0e-4530-84ca-33a07ba8e5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T08:37:23.177896Z",
     "iopub.status.busy": "2023-08-22T08:37:23.177690Z",
     "iopub.status.idle": "2023-08-22T08:37:26.953986Z",
     "shell.execute_reply": "2023-08-22T08:37:26.953158Z",
     "shell.execute_reply.started": "2023-08-22T08:37:23.177879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([314, 50])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "test['length'] = test['sentence'].str.split().str.len()\n",
    "test = test[test['length'] > seq_len].copy().reset_index(drop=True)\n",
    "X = tokenizer(test['sentence'].to_list(), padding=True,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b37cae3-20d2-4c17-8ae0-49ab862120ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-22T08:37:26.955495Z",
     "iopub.status.busy": "2023-08-22T08:37:26.955159Z",
     "iopub.status.idle": "2023-08-22T08:37:28.433275Z",
     "shell.execute_reply": "2023-08-22T08:37:28.431937Z",
     "shell.execute_reply.started": "2023-08-22T08:37:26.955469Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 314 0.0%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i, \u001b[38;5;28mlen\u001b[39m(test), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     weak_mwe_distance\u001b[38;5;241m.\u001b[39mextend(\u001b[43mmwe_distance_interaction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweak_mwe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     57\u001b[0m     strong_mwe_distance\u001b[38;5;241m.\u001b[39mextend(mwe_distance_interaction(X[i], row, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrong_mwe\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     59\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(weak_mwe_distance, columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposdis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweak_mwe_distance1.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 40\u001b[0m, in \u001b[0;36mmwe_distance_interaction\u001b[0;34m(encoded_row, row, col)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m mwe \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m seq_len]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     39\u001b[0m                     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m                 iv \u001b[38;5;241m=\u001b[39m \u001b[43minteraction_value_di\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmwe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmwe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m                 iv_mwe\u001b[38;5;241m.\u001b[39mappend([iv, \u001b[38;5;28mabs\u001b[39m((mwe[i]\u001b[38;5;241m-\u001b[39mmwe[j])),mwe])\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iv_mwe\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36minteraction_value_di\u001b[0;34m(model, X, tokens)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minteraction_value_di\u001b[39m(model, X, tokens):\n\u001b[1;32m      8\u001b[0m     token1, token2 \u001b[38;5;241m=\u001b[39m tokens\n\u001b[0;32m----> 9\u001b[0m     AB \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     X_t1 \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     12\u001b[0m     X_t1[token1] \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39munk_token_id\n",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m, in \u001b[0;36mget_prediction_softmax\u001b[0;34m(model, X)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_prediction_softmax\u001b[39m(model, X):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scipy\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py:843\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    840\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer)\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    845\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X):\n",
    "    return scipy.special.softmax(model(X).logits[-1].detach().numpy())\n",
    "\n",
    "def interaction_value_di(model, X, tokens):\n",
    "    token1, token2 = tokens\n",
    "    AB = get_prediction_softmax(model, X)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[token2] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2)\n",
    "\n",
    "    \n",
    "    X_t12 = X.clone()\n",
    "    X_t12[token2] = tokenizer.unk_token_id\n",
    "    X_t12[token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    val = np.linalg.norm(val)\n",
    "    return val\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col):\n",
    "    iv_mwe = []\n",
    "    # col = 'weak_mwe' | 'strong_mwe'\n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "                    iv = interaction_value_di(model, encoded_row, [mwe[i], mwe[j]])\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-mwe[j])),mwe])\n",
    "    return iv_mwe\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "# weak_mwe_distance = Parallel(n_jobs=num_cores)(\n",
    "#     delayed(mwe_distance_interaction)(X[i], row, 'weak_mwe') for i, row in test.iterrows())\n",
    "\n",
    "# strong_mwe_distance = Parallel(n_jobs=num_cores)(\n",
    "#     delayed(mwe_distance_interaction)(X[i], row, 'strong_mwe') for i, row in test.iterrows())\n",
    "\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    if i%100==0:\n",
    "        print(i, len(test), f'{i*100/len(test)}%')\n",
    "    weak_mwe_distance.extend(mwe_distance_interaction(X[i], row, 'weak_mwe'))\n",
    "    strong_mwe_distance.extend(mwe_distance_interaction(X[i], row, 'strong_mwe'))\n",
    "\n",
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore']).to_pickle('weak_mwe_distance1.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore']).to_pickle('strong_mwe_distance1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd809d-0069-4808-96e3-d39aae242337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b85413-4a72-49bd-9108-e0c902b2a49e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.433774Z",
     "iopub.status.idle": "2023-08-22T08:37:28.434016Z",
     "shell.execute_reply": "2023-08-22T08:37:28.433914Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.433902Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"Fetching Average Distance...\")\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_interaction(encoded_row, row):\n",
    "    interactions = []\n",
    "    for j in range( seq_len):\n",
    "        for k in range(j+1, seq_len):\n",
    "\n",
    "            if j+k >= seq_len:\n",
    "                continue\n",
    "            if encoded_row[j] == tokenizer.unk_token_id or encoded_row[k] == tokenizer.unk_token_id:\n",
    "                continue\n",
    "            \n",
    "            iv = interaction_value_di(model, encoded_row, [j, k])\n",
    "            interactions.append([iv, abs(k -j),[j,k]])\n",
    "    return interactions\n",
    "\n",
    "average_distance = []\n",
    "# start_time =  datetime.datetime.now()\n",
    "# for i, row in test.iterrows():\n",
    "#     # if i%9 == 0 or i<=100:\n",
    "#     print(i, len(test), f'{i*100/len(test)}%', (datetime.datetime.now() - start_time).seconds)\n",
    "#     start_time = datetime.datetime.now()\n",
    "#     average_distance.extend(calculate_interaction(X[i], row))\n",
    "\n",
    "num_cores = 9\n",
    "for x in range(0, len(test), 99):\n",
    "    print(x, len(test), f'{100*x/len(test)}%')\n",
    "    average_distance.extend(Parallel(n_jobs=num_cores)(\n",
    "        delayed(calculate_interaction)(X[i], row) for i, row in tqdm(test[x:x+99].iterrows())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79101d8-f90d-40ba-b007-e522fb779c68",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.435064Z",
     "iopub.status.idle": "2023-08-22T08:37:28.435334Z",
     "shell.execute_reply": "2023-08-22T08:37:28.435213Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.435201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_dist = [item for sublist in average_distance for item in sublist]\n",
    "pd.DataFrame(avg_dist, columns = ['I', 'posdis', 'ignore']).to_pickle('average_distance1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297cd3b-ad2c-4670-912e-565d8ceb34e9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.436743Z",
     "iopub.status.idle": "2023-08-22T08:37:28.436990Z",
     "shell.execute_reply": "2023-08-22T08:37:28.436885Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.436873Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# avg_dist = [item for sublist in average_distance for item in sublist]\n",
    "\n",
    "def plot_mean(avg_path='average_distance.pkl', weak_path='weak_mwe_distance.pkl', strong_path='strong_mwe_distance.pkl'):\n",
    "    avg_dist = pd.read_pickle(avg_path)\n",
    "    avg_df = pd.DataFrame(avg_dist, columns = ['I', 'posdis', 'ignore']).groupby('posdis')['I'].agg( ['mean', 'count', 'std']).rename(columns = {'mean':'avg_mean', 'count':'avg_count', 'std':'avg_std'})\n",
    "    weak_mwe_distance = pd.read_pickle(weak_path)\n",
    "    weak_mwe_df = pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore']).groupby('posdis')['I'].agg( ['mean', 'count', 'std']).rename(columns = {'mean':'weak_mean', 'count':'weak_count', 'std':'weak_std'})\n",
    "    weak_mwe_df = weak_mwe_df.drop(0)\n",
    "    strong_mwe_distance = pd.read_pickle(strong_path)\n",
    "    strong_mwe_df = pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore']).groupby('posdis')['I'].agg( ['mean', 'count', 'std']).rename(columns = {'mean':'strong_mean', 'count':'strong_count', 'std':'strong_std'})\n",
    "    abc = pd.concat([avg_df, weak_mwe_df, strong_mwe_df], axis=1)\n",
    "    display(abc)\n",
    "    abc = abc[abc['weak_count'] >=50]\n",
    "    abc[['avg_mean', 'weak_mean']].plot()\n",
    "    plt.show()\n",
    "plot_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e62896-a71a-43cb-80ff-d91887fa8319",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.438004Z",
     "iopub.status.idle": "2023-08-22T08:37:28.438267Z",
     "shell.execute_reply": "2023-08-22T08:37:28.438161Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.438149Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boxplot_posdis(avg_path='average_distance.pkl', weak_path='weak_mwe_distance.pkl', strong_path='strong_mwe_distance.pkl'):\n",
    "    avg_dist = pd.read_pickle(avg_path)\n",
    "    weak_mwe_distance = pd.read_pickle(weak_path)\n",
    "    strong_mwe_distance = pd.read_pickle(strong_path)\n",
    "    \n",
    "    avg_df = pd.DataFrame(avg_dist, columns = ['I', 'posdis', 'ignore']).drop(columns = ['ignore']).assign(Location='avg')\n",
    "    weak_df = pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore']).drop(columns = ['ignore']).assign(Location='weak')\n",
    "    weak_df = weak_df[weak_df['posdis']!=0].copy()\n",
    "    strong_df = pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore']).drop(columns = ['ignore']).assign(Location='strong')\n",
    "    cdf = pd.concat([avg_df, weak_df])#, strong_df])    \n",
    "    cdf = cdf[cdf['posdis']<=5].copy()\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    ax = sns.boxplot(x=\"posdis\", y='I',hue=\"Location\", data=cdf)     #hue=\"Letter\",\n",
    "    plt.show()\n",
    "\n",
    "boxplot_posdis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef6dcf-f9cc-49c9-aac8-5f3028f00cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7f603-c90c-44aa-abdf-17d1a898a078",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.439659Z",
     "iopub.status.idle": "2023-08-22T08:37:28.439922Z",
     "shell.execute_reply": "2023-08-22T08:37:28.439816Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.439803Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avg_df.boxplot(column = ['I'], by = 'posdis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c53f1-8ec6-4f39-b60b-f528a61b49a6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.441422Z",
     "iopub.status.idle": "2023-08-22T08:37:28.441687Z",
     "shell.execute_reply": "2023-08-22T08:37:28.441579Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.441567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc9087-e4a3-4587-82bc-e851a4b6b7fa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.442365Z",
     "iopub.status.idle": "2023-08-22T08:37:28.442610Z",
     "shell.execute_reply": "2023-08-22T08:37:28.442504Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.442493Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(X[0:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d853449-b783-4087-8d90-c4c0c413fde0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.443780Z",
     "iopub.status.idle": "2023-08-22T08:37:28.444037Z",
     "shell.execute_reply": "2023-08-22T08:37:28.443930Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.443918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[x for x in testlist_text if 'Although he wrote' in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95911c8d-5b2c-4b78-a774-0eaaf4a905b0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.445428Z",
     "iopub.status.idle": "2023-08-22T08:37:28.445700Z",
     "shell.execute_reply": "2023-08-22T08:37:28.445592Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.445579Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_mode = 1 # [norm of softmax ]\n",
    "\n",
    "\n",
    "# TODO: Fix the prediction fn. Might have to incorporate\n",
    "#  target variable to get the logit for the target variable instead of max\n",
    "predict_fn = llm_helper.get_prediction_fn(model, pred_mode = pred_mode)\n",
    "\n",
    "\n",
    "\n",
    "torch.no_grad()\n",
    "\n",
    "obj = RegressionGame(X = X[0:k], y=None, function = predict_fn, transform = torch.as_tensor)\n",
    "\n",
    "X_samp = X[k:(N+k)]\n",
    "\n",
    "shapley_values = np.empty((0, X.shape[1]))\n",
    "partial_residuals = np.empty((0, X.shape[1]))\n",
    "games = np.empty((0, 2 ** X.shape[1]))\n",
    "\n",
    "print(\"SHape\")\n",
    "print(X.shape[1])\n",
    "\n",
    "print(\"  ..ok!\")\n",
    "print(\"Generating explanations..\")\n",
    "\n",
    "for i in range(0, N):\n",
    "    example_row = X_samp[i,:].reshape((1,X_samp.shape[1]))\n",
    "    game = obj.getKernelSHAPGame(example_row)\n",
    "    games = np.append(games, game.reshape((1,game.shape[0])), axis = 0)\n",
    "    results, residualGame, origGame = getShapleyProjection(game)\n",
    "    shapley_values = np.append(shapley_values,\n",
    "                               np.array([np.flip(results[-1])]), axis=0)\n",
    "    partial_residuals = np.append(partial_residuals,\n",
    "                                  np.array([np.flip(norm(residualGame, axis = 0)/norm(origGame, axis = 0))]), axis = 0)\n",
    "    print(\"%s/%s samples done.\" % (i+1, N))\n",
    "    \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        pd.DataFrame(X_samp).to_csv('data/llm_input.csv')\n",
    "        pd.DataFrame(shapley_values).to_csv('data/llm_shapley_values.csv')\n",
    "        pd.DataFrame(partial_residuals).to_csv('data/llm_partial_residuals.csv')\n",
    "\n",
    "print(\" Explanations saved to data/llm_*.csv!\")\n",
    "\n",
    "pd.DataFrame(X_samp).to_csv('data/llm_input.csv')\n",
    "pd.DataFrame(shapley_values).to_csv('data/llm_shapley_values.csv')\n",
    "pd.DataFrame(partial_residuals).to_csv('data/llm_partial_residuals.csv')\n",
    "\n",
    "\"\"\"\n",
    "TODO: \n",
    "1. How to get base line features for the text generation process? \n",
    "2. What is the appropriate metric for shapley score. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22e10c-cfff-4263-bc32-f15b5e366605",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.446772Z",
     "iopub.status.idle": "2023-08-22T08:37:28.447167Z",
     "shell.execute_reply": "2023-08-22T08:37:28.446990Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.446972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85b61f-b7d2-4053-b597-712cdac20257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8404e0-d4f3-4fad-b020-58ad8f706e8f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.448961Z",
     "iopub.status.idle": "2023-08-22T08:37:28.449330Z",
     "shell.execute_reply": "2023-08-22T08:37:28.449208Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.449191Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_fn = llm_helper.get_prediction_fn(model, pred_mode = 1)\n",
    "\n",
    "\n",
    "\n",
    "# torch.no_grad()\n",
    "\n",
    "\n",
    "# # obj = RegressionGame(X = X[0:k], y=y[0:k], function = predict_fn, transform = torch.as_tensor)\n",
    "# obj = RegressionGame(X = X[0:k], y=None, function = predict_fn, transform = torch.as_tensor)\n",
    "\n",
    "# X_samp = X[k:(N+k)]\n",
    "\n",
    "# shapley_values = np.empty((0, X.shape[1]))\n",
    "# partial_residuals = np.empty((0, X.shape[1]))\n",
    "# games = np.empty((0, 2 ** X.shape[1]))\n",
    "\n",
    "# print(\"SHape\")\n",
    "# print(X.shape[1])\n",
    "\n",
    "# print(\"  ..ok!\")\n",
    "# print(\"Generating explanations..\")\n",
    "\n",
    "# for i in range(0, N):\n",
    "#     example_row = X_samp[i,:].reshape((1,X_samp.shape[1]))\n",
    "#     game = obj.getKernelSHAPGame(example_row)\n",
    "#     games = np.append(games, game.reshape((1,game.shape[0])), axis = 0)\n",
    "#     results, residualGame, origGame = getShapleyProjection(game)\n",
    "#     shapley_values = np.append(shapley_values,\n",
    "#                                np.array([np.flip(results[-1])]), axis=0)\n",
    "#     partial_residuals = np.append(partial_residuals,\n",
    "#                                   np.array([np.flip(norm(residualGame, axis = 0)/norm(origGame, axis = 0))]), axis = 0)\n",
    "#     print(\"%s/%s samples done.\" % (i+1, N))\n",
    "\n",
    "# print(\" Explanations saved to data/llm_*.csv!\")\n",
    "\n",
    "# pd.DataFrame(X_samp).to_csv('data/llm_input.csv')\n",
    "# pd.DataFrame(shapley_values).to_csv('data/llm_shapley_values.csv')\n",
    "# pd.DataFrame(partial_residuals).to_csv('data/llm_partial_residuals.csv')\n",
    "\n",
    "# \"\"\"\n",
    "# TODO: \n",
    "# 1. How to get base line features for the text generation process? \n",
    "# 2. What is the appropriate metric for shapley score. \n",
    "# 3. issue here, check why I was not able to make k = 40. model is probably not able to take more than 25. Check\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbff028-32d9-4cbc-94d3-9574243e1d06",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.450821Z",
     "iopub.status.idle": "2023-08-22T08:37:28.451094Z",
     "shell.execute_reply": "2023-08-22T08:37:28.450985Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.450971Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a1e48-e0a0-4692-8905-7a8322da0dc9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-22T08:37:28.452687Z",
     "iopub.status.idle": "2023-08-22T08:37:28.452966Z",
     "shell.execute_reply": "2023-08-22T08:37:28.452855Z",
     "shell.execute_reply.started": "2023-08-22T08:37:28.452842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9c1e8-b53f-4f3e-bc3c-4d579f6a6f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
