{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af294e1d-0794-4a9a-b981-69ad7c9d4001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T21:40:55.214583Z",
     "iopub.status.busy": "2023-08-24T21:40:55.214348Z",
     "iopub.status.idle": "2023-08-24T21:40:56.860088Z",
     "shell.execute_reply": "2023-08-24T21:40:56.859468Z",
     "shell.execute_reply.started": "2023-08-24T21:40:55.214561Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dependencies..\n",
      "Seq Length: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing dependencies..\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "from scipy.linalg import norm \n",
    "import pandas as pd\n",
    "import os\n",
    "# from util import RegressionGame\n",
    "# from util_sparse import getShapleyProjection\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# import shap\n",
    "# import llm_helper\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "if not os.path.exists('__dcache__'):\n",
    "    os.makedirs('__dcache__')\n",
    "\n",
    "# N = 6000\n",
    "# print(\"Explanation count: %s\" % N)\n",
    "# k = 100\n",
    "# print(\"SHAP sample count: %s\" % k)\n",
    "seq_len = 50\n",
    "print(\"Seq Length: %s\" % seq_len)\n",
    "\n",
    "# Setup\n",
    "np.random.seed(1)\n",
    "# model = llm_helper.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da144645-241a-4cde-991e-f73e335410db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d149b369-2ff7-4d2f-a42a-4cde9979fee4",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59cbc80a-210c-40ff-89aa-ce9da3dcc78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T21:40:56.862119Z",
     "iopub.status.busy": "2023-08-24T21:40:56.861679Z",
     "iopub.status.idle": "2023-08-24T21:41:09.142344Z",
     "shell.execute_reply": "2023-08-24T21:41:09.141555Z",
     "shell.execute_reply.started": "2023-08-24T21:40:56.862090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "if cuda:\n",
    "    model = BertForMaskedLM.from_pretrained('bert-base-uncased').cuda()\n",
    "else:\n",
    "    model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "test = pd.read_pickle('../mwe_tagger/wikitext_train_gpt.pkl')\n",
    "test['length'] = test['sentence'].str.split().str.len()\n",
    "test = test[~((test['weak_mwe'].str.len()==0) & (test['strong_mwe'].str.len()==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e38b72-b014-4954-8199-dd25aaeb479c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T21:41:09.143355Z",
     "iopub.status.busy": "2023-08-24T21:41:09.143164Z",
     "iopub.status.idle": "2023-08-24T21:41:09.166894Z",
     "shell.execute_reply": "2023-08-24T21:41:09.165908Z",
     "shell.execute_reply.started": "2023-08-24T21:41:09.143338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81605516-9ab4-4f75-a23d-7354deb625dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T21:41:09.167759Z",
     "iopub.status.busy": "2023-08-24T21:41:09.167558Z",
     "iopub.status.idle": "2023-08-24T21:41:09.193884Z",
     "shell.execute_reply": "2023-08-24T21:41:09.193077Z",
     "shell.execute_reply.started": "2023-08-24T21:41:09.167741Z"
    }
   },
   "outputs": [],
   "source": [
    "# model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c1bfd3-2d37-440e-99ad-9bee13415fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-24T21:45:16.635770Z",
     "iopub.status.busy": "2023-08-24T21:45:16.635491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                       | 7/58830 [00:33<95:46:56,  5.86s/it]"
     ]
    }
   ],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X, masked_tokens):\n",
    "    logit_x = model(X).logits\n",
    "    nec = []\n",
    "    for i in range(len(masked_tokens)):\n",
    "        nec.append(logit_x[i][masked_tokens[i]])\n",
    "    nec = torch.stack(nec)\n",
    "    if cuda:\n",
    "        return torch.softmax(nec, dim=1)\n",
    "    else:\n",
    "        return scipy.special.softmax(model(X).logits[masked_token].detach().numpy(), axis=1)\n",
    "\n",
    "def interaction_value_di(model, X, tokens , masked_tokens):\n",
    "    token1, token2 = tokens\n",
    "    \n",
    "    # token_next = max(token1, token2) + 1\n",
    "    AB = get_prediction_softmax(model, X, masked_tokens)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[:,token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1, masked_tokens)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[:,token1] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2, masked_tokens)\n",
    "\n",
    "    X_t12 = X.clone()\n",
    "    X_t12[:,token2] = tokenizer.unk_token_id\n",
    "    X_t12[:,token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12, masked_tokens)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    if cuda:\n",
    "        val = torch.linalg.norm(val, dim=1).cpu()\n",
    "    else:\n",
    "        val = np.linalg.norm(val, axis=1)\n",
    "    \n",
    "    if cuda:\n",
    "        return val.detach(), masked_tokens\n",
    "    else:\n",
    "        return val, token_next\n",
    "\n",
    "\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col, row_number):\n",
    "    iv_mwe = []\n",
    "    \n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "\n",
    "                    ogs = []\n",
    "                    ks = []\n",
    "                    for k in range(len(encoded_row)):\n",
    "                        if k != mwe[i] -1 and k!= mwe[j] - 1 and abs(k - max(mwe[i]-1, mwe[j]-1))<=12:\n",
    "                            og = encoded_row.clone()\n",
    "                            og[k] = tokenizer.mask_token_id\n",
    "                            ogs.append(og)\n",
    "                            ks.append(k)\n",
    "                    ogs = torch.stack(ogs)\n",
    "                    iv = interaction_value_di(model, ogs, [mwe[i]-1, mwe[j]-1], ks)\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-1-(mwe[j]-1))),mwe, row_number, i, j, ks])\n",
    "\n",
    "    return iv_mwe\n",
    "\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows(), total=test.shape[0]):\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    if len(row['weak_mwe']) != 0:\n",
    "        weak_mwe_distance.append(mwe_distance_interaction(abc, row, 'weak_mwe', row_number))\n",
    "    \n",
    "    if len(row['strong_mwe']) != 0:\n",
    "        strong_mwe_distance.append(mwe_distance_interaction(abc, row, 'strong_mwe', row_number))\n",
    "    \n",
    "    del abc\n",
    "\n",
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('weak_mwe_distance_3d_mlm.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('strong_mwe_distance_3d_mlm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9dd24-faaf-4fe2-8980-2d9ac6580422",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.515238Z",
     "iopub.status.idle": "2023-08-24T21:41:10.515506Z",
     "shell.execute_reply": "2023-08-24T21:41:10.515398Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.515386Z"
    }
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X, masked_tokens):\n",
    "    logit_x = model(X).logits\n",
    "    nec = []\n",
    "    for i in range(len(masked_tokens)):\n",
    "        nec.append(logit_x[i][masked_tokens[i]])\n",
    "    nec = torch.stack(nec)\n",
    "    if cuda:\n",
    "        return torch.softmax(model(X).logits[masked_token], dim=1)\n",
    "    else:\n",
    "        return scipy.special.softmax(model(X).logits[masked_token].detach().numpy(), axis=1)\n",
    "\n",
    "def interaction_value_di(model, X, tokens , masked_tokens):\n",
    "    token1, token2 = tokens\n",
    "    \n",
    "    # token_next = max(token1, token2) + 1\n",
    "    AB = get_prediction_softmax(model, X, masked_tokens)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[:,token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1, masked_tokens)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[:,token1] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2, masked_tokens)\n",
    "\n",
    "    X_t12 = X.clone()\n",
    "    X_t12[:,token2] = tokenizer.unk_token_id\n",
    "    X_t12[:,token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12, masked_tokens)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    if cuda:\n",
    "        val = torch.linalg.norm(val, dim=1).cpu()\n",
    "    else:\n",
    "        val = np.linalg.norm(val, axis=1)\n",
    "    \n",
    "    if cuda:\n",
    "        return val.detach(), masked_tokens\n",
    "    else:\n",
    "        return val, token_next\n",
    "\n",
    "\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col, row_number):\n",
    "    iv_mwe = []\n",
    "    \n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "\n",
    "                    ogs = []\n",
    "                    ks = []\n",
    "                    for k in range(len(encoded_row)):\n",
    "                        if k != mwe[i] -1 and k!= mwe[j] - 1 and abs(k - max(mwe[i]-1, mwe[j]-1))<=12:\n",
    "                            og = encoded_row.clone()\n",
    "                            og[k] = tokenizer.mask_token_id\n",
    "                            ogs.append(og)\n",
    "                            ks.append(k)\n",
    "                    ogs = torch.stack(ogs)\n",
    "                    iv = interaction_value_di(model, ogs, [mwe[i]-1, mwe[j]-1], ks)\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-1-(mwe[j]-1))),mwe, row_number, i, j, ks])\n",
    "\n",
    "    return iv_mwe\n",
    "\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows(), total=test.shape[0]):\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    if len(row['weak_mwe']) != 0:\n",
    "        weak_mwe_distance.append(mwe_distance_interaction(abc, row, 'weak_mwe', row_number))\n",
    "    \n",
    "    if len(row['strong_mwe']) != 0:\n",
    "        strong_mwe_distance.append(mwe_distance_interaction(abc, row, 'strong_mwe', row_number))\n",
    "    \n",
    "    del abc\n",
    "\n",
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('weak_mwe_distance_3d_mlm.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('strong_mwe_distance_3d_mlm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4a83a-40dc-4a87-a51e-bfd87700f957",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.515238Z",
     "iopub.status.idle": "2023-08-24T21:41:10.515506Z",
     "shell.execute_reply": "2023-08-24T21:41:10.515398Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.515386Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import datetime\n",
    "# # os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# print(\"Fetching Average Distance...\")\n",
    "# cuda = True\n",
    "\n",
    "\n",
    "# def calculate_interaction(encoded_row, row, row_number):\n",
    "#     interactions = []\n",
    "#     encoded_len = len(encoded_row)\n",
    "#     for j in range( min(seq_len, encoded_len)):\n",
    "#         for k in range(j+1, min(seq_len, encoded_len, j+8)):\n",
    "#             if j+k >= encoded_len:\n",
    "#                 continue\n",
    "#             if encoded_row[j] == tokenizer.unk_token_id or encoded_row[k] == tokenizer.unk_token_id:\n",
    "#                 continue\n",
    "            \n",
    "#             iv = interaction_value_di(model, encoded_row, [j, k])\n",
    "#             interactions.append([iv, abs(k -j),row_number, [j,k]])\n",
    "#     return interactions\n",
    "\n",
    "# average_distance = []\n",
    "# start_time =  datetime.datetime.now()\n",
    "\n",
    "# for row_number, row in tqdm(test.iterrows()):\n",
    "#     abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "#     if cuda:\n",
    "#         abc = abc.cuda()\n",
    "\n",
    "    \n",
    "#     average_distance.extend(calculate_interaction(abc, row, row_number))\n",
    "\n",
    "\n",
    "# pd.DataFrame(average_distance, columns = ['I', 'posdis', 'row_number', 'word_pair']).to_pickle('average_distance_3d_mlm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584b85a-0159-4ff7-b5a7-5419997adce0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.516448Z",
     "iopub.status.idle": "2023-08-24T21:41:10.516715Z",
     "shell.execute_reply": "2023-08-24T21:41:10.516607Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.516596Z"
    }
   },
   "outputs": [],
   "source": [
    "asdasdasd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e483ca-93dc-4903-912c-e9db286c64c2",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f68b9-de64-46ad-b985-22bf0bd9dec3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.518202Z",
     "iopub.status.idle": "2023-08-24T21:41:10.518596Z",
     "shell.execute_reply": "2023-08-24T21:41:10.518426Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.518408Z"
    }
   },
   "outputs": [],
   "source": [
    "cuda = True\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "if cuda:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "# test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "test = pd.read_pickle('../mwe_tagger/wikitext_train_gpt.pkl')\n",
    "test['length'] = test['sentence'].str.split().str.len()\n",
    "test = test[~((test['weak_mwe'].str.len()==0) & (test['strong_mwe'].str.len()==0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80dc2a8-33f4-4de4-9d7d-ce7cd2447dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6f077-94e3-4be8-b66b-6fb387ff45bc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.519658Z",
     "iopub.status.idle": "2023-08-24T21:41:10.519969Z",
     "shell.execute_reply": "2023-08-24T21:41:10.519851Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.519834Z"
    }
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X, token_next):\n",
    "    if cuda:\n",
    "        return torch.softmax(model(X).logits[token_next:], dim=1)\n",
    "    else:\n",
    "        return scipy.special.softmax(model(X).logits[token_next:].detach().numpy(), axis=1)\n",
    "\n",
    "def interaction_value_di(model, X, tokens):\n",
    "    token1, token2 = tokens\n",
    "    \n",
    "    token_next = max(token1, token2) + 1\n",
    "    AB = get_prediction_softmax(model, X, token_next)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1, token_next)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[token2] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2, token_next)\n",
    "\n",
    "    X_t12 = X.clone()\n",
    "    X_t12[token2] = tokenizer.unk_token_id\n",
    "    X_t12[token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12, token_next)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    if cuda:\n",
    "        val = torch.linalg.norm(val, dim=1).cpu()\n",
    "    else:\n",
    "        val = np.linalg.norm(val, axis=1)\n",
    "    \n",
    "    del X_t1, X_t2, X_t12,  AB, A, B, phi\n",
    "    if cuda:\n",
    "        return val.detach(), token_next\n",
    "    else:\n",
    "        return val, token_next\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col, row_number):\n",
    "    iv_mwe = []\n",
    "    # col = 'weak_mwe' | 'strong_mwe'\n",
    "    \n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "                    \n",
    "                    iv = interaction_value_di(model, encoded_row, [mwe[i]-1, mwe[j]-1])\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-1-(mwe[j]-1))),mwe, row_number, i, j])\n",
    "    # print(torch.cuda.memory_allocated()/(1024*1024*1024))\n",
    "    return iv_mwe\n",
    "\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows(), total=test.shape[0]):\n",
    "        # print(row_number, len(test), f'{row_number*100/len(test)}%')\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    # print(\"Start\",torch.cuda.memory_allocated()/(1024*1024*1024))\n",
    "    # weak_mwe_distance.extend(mwe_distance_interaction(abc, row, 'weak_mwe', row_number))\n",
    "    # strong_mwe_distance.extend(mwe_distance_interaction(abc, row, 'strong_mwe', row_number))\n",
    "    if len(row['weak_mwe']) != 0:\n",
    "        weak_mwe_distance.append(mwe_distance_interaction(abc, row, 'weak_mwe', row_number))\n",
    "    \n",
    "    if len(row['strong_mwe']) != 0:\n",
    "        strong_mwe_distance.append(mwe_distance_interaction(abc, row, 'strong_mwe', row_number))\n",
    "    \n",
    "    # print(torch.cuda.memory_allocated()/(1024*1024*1024))\n",
    "    del abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f621ce3-0846-456e-8f7d-f9788b80d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55934a4b-e781-4db1-8c1e-0d97508aaa20",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.520822Z",
     "iopub.status.idle": "2023-08-24T21:41:10.521087Z",
     "shell.execute_reply": "2023-08-24T21:41:10.520980Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.520968Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(strong_mwe_distance, open('test1.pkl','wb'))\n",
    "pickle.dump(weak_mwe_distance, open('test2.pkl','wb'))\n",
    "weak_mwe_distance = [y for x in weak_mwe_distance for y in x]\n",
    "strong_mwe_distance = [y for x in strong_mwe_distance for y in x]\n",
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('weak_mwe_distance_3d_full.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token']).to_pickle('strong_mwe_distance_3d_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478f6ae-f304-4949-95fe-83144894a395",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.522689Z",
     "iopub.status.idle": "2023-08-24T21:41:10.523048Z",
     "shell.execute_reply": "2023-08-24T21:41:10.522911Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.522898Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore', 'row_number', 'first_token', 'second_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde2911-b39b-4af9-9c57-040d2e467520",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.525525Z",
     "iopub.status.idle": "2023-08-24T21:41:10.525852Z",
     "shell.execute_reply": "2023-08-24T21:41:10.525716Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.525704Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"Fetching Average Distance...\")\n",
    "cuda = True\n",
    "\n",
    "\n",
    "def calculate_interaction(encoded_row, row, row_number):\n",
    "    interactions = []\n",
    "    encoded_len = len(encoded_row)\n",
    "    for j in range( min(seq_len, encoded_len)):\n",
    "        for k in range(j+1, min(seq_len, encoded_len, j+10)):\n",
    "            if j+k >= encoded_len:\n",
    "                continue\n",
    "            if encoded_row[j] == tokenizer.unk_token_id or encoded_row[k] == tokenizer.unk_token_id:\n",
    "                continue\n",
    "            \n",
    "            iv = interaction_value_di(model, encoded_row, [j, k])\n",
    "            interactions.append([iv, abs(k -j),row_number, [j,k]])\n",
    "    return interactions\n",
    "\n",
    "average_distance = []\n",
    "start_time =  datetime.datetime.now()\n",
    "# min_row = average_distance[-1][2]\n",
    "# print(min_row)\n",
    "\n",
    "for row_number, row in tqdm(test.iterrows(), total=test.shape[0]):\n",
    "    # if i%9 == 0 or i<=100:\n",
    "    # print(i, len(test), f'{i*100/len(test)}%', (datetime.datetime.now() - start_time).seconds)\n",
    "    # start_time = datetime.datetime.now()\n",
    "    abc =  tokenizer(row['sentence'], padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    if cuda:\n",
    "        abc = abc.cuda()\n",
    "\n",
    "    \n",
    "    average_distance.extend(calculate_interaction(abc, row, row_number))\n",
    "    if row_number % 1000 == 0:\n",
    "        print(len(average_distance))\n",
    "\n",
    "        if len(average_distance) >= 457902:\n",
    "            break\n",
    "\n",
    "# from joblib import Parallel, delayed\n",
    "# \n",
    "# num_cores = 9\n",
    "# for x in range(0, len(test), 99):\n",
    "#     print(x, len(test), f'{100*x/len(test)}%')\n",
    "#     fabc =  lambda x : tokenizer(x, padding=False,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids[0]\n",
    "    \n",
    "#     average_distance.extend(Parallel(n_jobs=num_cores)(\n",
    "#         delayed(calculate_interaction)(fabc(row['sentence']), row, i) for i, row in tqdm(test[x:x+99].iterrows())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3989a8-a69f-4549-bd62-dd3ef802ab41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6829488-fe87-49e0-b606-a39640d7f393",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.526961Z",
     "iopub.status.idle": "2023-08-24T21:41:10.527220Z",
     "shell.execute_reply": "2023-08-24T21:41:10.527114Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.527103Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(average_distance, columns = ['I', 'posdis', 'row_number', 'word_pair']).to_pickle('average_distance_3d_full.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91405974-39af-486a-a013-6b2b4c313d57",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b2769-b5ee-46fc-b063-8ff994687038",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.529351Z",
     "iopub.status.idle": "2023-08-24T21:41:10.529949Z",
     "shell.execute_reply": "2023-08-24T21:41:10.529733Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.529717Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "a = np.unique([y[-1]-y[0] for x in test['_'].to_list() for y in x], return_counts=True)\n",
    "print({x:y for x,y in zip(a[0], a[1][::-1].cumsum()[::-1])})\n",
    "print(np.unique([z for x in test['_'].to_list() for y in x for z in y], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbd21c-dcc2-4148-9f5c-a9e78d80e7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbcc8062-e2f8-4fbc-88c9-40a582d4041d",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725b98f-ac0e-4530-84ca-33a07ba8e5f8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.530841Z",
     "iopub.status.idle": "2023-08-24T21:41:10.531107Z",
     "shell.execute_reply": "2023-08-24T21:41:10.531001Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.530989Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "test = pd.read_pickle('../mwe_tagger/fsample.pkl')\n",
    "test['length'] = test['sentence'].str.split().str.len()\n",
    "test = test[test['length'] > seq_len].copy().reset_index(drop=True)\n",
    "X = tokenizer(test['sentence'].to_list(), padding=True,  truncation=True, max_length=seq_len, return_tensors ='pt').input_ids\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b37cae3-20d2-4c17-8ae0-49ab862120ca",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.532800Z",
     "iopub.status.idle": "2023-08-24T21:41:10.533079Z",
     "shell.execute_reply": "2023-08-24T21:41:10.532965Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.532953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed\n",
    "# num_cores = 10\n",
    "\n",
    "def get_prediction_softmax(model, X):\n",
    "    return scipy.special.softmax(model(X).logits[-1].detach().numpy())\n",
    "\n",
    "def interaction_value_di(model, X, tokens):\n",
    "    token1, token2 = tokens\n",
    "    AB = get_prediction_softmax(model, X)\n",
    "    \n",
    "    X_t1 = X.clone()\n",
    "    X_t1[token1] = tokenizer.unk_token_id\n",
    "    A = get_prediction_softmax(model, X_t1)\n",
    "    \n",
    "    X_t2 = X.clone()\n",
    "    X_t2[token2] = tokenizer.unk_token_id\n",
    "    B = get_prediction_softmax(model, X_t2)\n",
    "\n",
    "    \n",
    "    X_t12 = X.clone()\n",
    "    X_t12[token2] = tokenizer.unk_token_id\n",
    "    X_t12[token1] = tokenizer.unk_token_id\n",
    "    phi = get_prediction_softmax(model, X_t12)\n",
    "    \n",
    "    val = AB - A - B + phi\n",
    "    val = np.linalg.norm(val)\n",
    "    return val\n",
    "\n",
    "def mwe_distance_interaction(encoded_row, row, col):\n",
    "    iv_mwe = []\n",
    "    # col = 'weak_mwe' | 'strong_mwe'\n",
    "    mwes = row[col]\n",
    "    for mwe in mwes:\n",
    "\n",
    "        for i in range(len(mwe)):\n",
    "            for j in range(len(mwe)):\n",
    "                if i > j:\n",
    "                    if len([x for x in mwe if x >= seq_len]) > 0:\n",
    "                        continue\n",
    "                    iv = interaction_value_di(model, encoded_row, [mwe[i]-1, mwe[j]-1])\n",
    "                    iv_mwe.append([iv, abs((mwe[i]-1-(mwe[j]-1))),mwe])\n",
    "    return iv_mwe\n",
    "weak_mwe_distance = []\n",
    "strong_mwe_distance = []\n",
    "\n",
    "# weak_mwe_distance = Parallel(n_jobs=num_cores)(\n",
    "#     delayed(mwe_distance_interaction)(X[i], row, 'weak_mwe') for i, row in test.iterrows())\n",
    "\n",
    "# strong_mwe_distance = Parallel(n_jobs=num_cores)(\n",
    "#     delayed(mwe_distance_interaction)(X[i], row, 'strong_mwe') for i, row in test.iterrows())\n",
    "\n",
    "\n",
    "for i, row in test.iterrows():\n",
    "    if i%100==0:\n",
    "        print(i, len(test), f'{i*100/len(test)}%')\n",
    "    weak_mwe_distance.extend(mwe_distance_interaction(X[i], row, 'weak_mwe'))\n",
    "    strong_mwe_distance.extend(mwe_distance_interaction(X[i], row, 'strong_mwe'))\n",
    "\n",
    "pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore']).to_pickle('weak_mwe_distance1.pkl')\n",
    "pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore']).to_pickle('strong_mwe_distance1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cd809d-0069-4808-96e3-d39aae242337",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b85413-4a72-49bd-9108-e0c902b2a49e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.534655Z",
     "iopub.status.idle": "2023-08-24T21:41:10.535155Z",
     "shell.execute_reply": "2023-08-24T21:41:10.534946Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.534926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print(\"Fetching Average Distance...\")\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_interaction(encoded_row, row):\n",
    "    interactions = []\n",
    "    for j in range( seq_len):\n",
    "        for k in range(j+1, seq_len):\n",
    "\n",
    "            if j+k >= seq_len:\n",
    "                continue\n",
    "            if encoded_row[j] == tokenizer.unk_token_id or encoded_row[k] == tokenizer.unk_token_id:\n",
    "                continue\n",
    "            \n",
    "            iv = interaction_value_di(model, encoded_row, [j, k])\n",
    "            interactions.append([iv, abs(k -j),[j,k]])\n",
    "    return interactions\n",
    "\n",
    "average_distance = []\n",
    "# start_time =  datetime.datetime.now()\n",
    "# for i, row in test.iterrows():\n",
    "#     # if i%9 == 0 or i<=100:\n",
    "#     print(i, len(test), f'{i*100/len(test)}%', (datetime.datetime.now() - start_time).seconds)\n",
    "#     start_time = datetime.datetime.now()\n",
    "#     average_distance.extend(calculate_interaction(X[i], row))\n",
    "\n",
    "num_cores = 9\n",
    "for x in range(0, len(test), 99):\n",
    "    print(x, len(test), f'{100*x/len(test)}%')\n",
    "    average_distance.extend(Parallel(n_jobs=num_cores)(\n",
    "        delayed(calculate_interaction)(X[i], row) for i, row in tqdm(test[x:x+99].iterrows())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79101d8-f90d-40ba-b007-e522fb779c68",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.536819Z",
     "iopub.status.idle": "2023-08-24T21:41:10.537147Z",
     "shell.execute_reply": "2023-08-24T21:41:10.537020Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.537001Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_dist = [item for sublist in average_distance for item in sublist]\n",
    "pd.DataFrame(avg_dist, columns = ['I', 'posdis', 'ignore']).to_pickle('average_distance1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297cd3b-ad2c-4670-912e-565d8ceb34e9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.538913Z",
     "iopub.status.idle": "2023-08-24T21:41:10.539260Z",
     "shell.execute_reply": "2023-08-24T21:41:10.539136Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.539121Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# avg_dist = [item for sublist in average_distance for item in sublist]\n",
    "\n",
    "def plot_mean(avg_path='average_distance.pkl', weak_path='weak_mwe_distance.pkl', strong_path='strong_mwe_distance.pkl'):\n",
    "    avg_dist = pd.read_pickle(avg_path)\n",
    "    avg_df = pd.DataFrame(avg_dist, columns = ['I', 'posdis', 'ignore']).groupby('posdis')['I'].agg( ['mean', 'count', 'std']).rename(columns = {'mean':'avg_mean', 'count':'avg_count', 'std':'avg_std'})\n",
    "    weak_mwe_distance = pd.read_pickle(weak_path)\n",
    "    weak_mwe_df = pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore']).groupby('posdis')['I'].agg( ['mean', 'count', 'std']).rename(columns = {'mean':'weak_mean', 'count':'weak_count', 'std':'weak_std'})\n",
    "    weak_mwe_df = weak_mwe_df.drop(0)\n",
    "    strong_mwe_distance = pd.read_pickle(strong_path)\n",
    "    strong_mwe_df = pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore']).groupby('posdis')['I'].agg( ['mean', 'count', 'std']).rename(columns = {'mean':'strong_mean', 'count':'strong_count', 'std':'strong_std'})\n",
    "    abc = pd.concat([avg_df, weak_mwe_df, strong_mwe_df], axis=1)\n",
    "    display(abc)\n",
    "    abc = abc[abc['weak_count'] >=50]\n",
    "    abc[['avg_mean', 'weak_mean']].plot()\n",
    "    plt.show()\n",
    "plot_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e62896-a71a-43cb-80ff-d91887fa8319",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.540746Z",
     "iopub.status.idle": "2023-08-24T21:41:10.541034Z",
     "shell.execute_reply": "2023-08-24T21:41:10.540925Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.540912Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boxplot_posdis(avg_path='average_distance.pkl', weak_path='weak_mwe_distance.pkl', strong_path='strong_mwe_distance.pkl'):\n",
    "    avg_dist = pd.read_pickle(avg_path)\n",
    "    weak_mwe_distance = pd.read_pickle(weak_path)\n",
    "    strong_mwe_distance = pd.read_pickle(strong_path)\n",
    "    \n",
    "    avg_df = pd.DataFrame(avg_dist, columns = ['I', 'posdis', 'ignore']).drop(columns = ['ignore']).assign(Location='avg')\n",
    "    weak_df = pd.DataFrame(weak_mwe_distance, columns = ['I', 'posdis', 'ignore']).drop(columns = ['ignore']).assign(Location='weak')\n",
    "    weak_df = weak_df[weak_df['posdis']!=0].copy()\n",
    "    strong_df = pd.DataFrame(strong_mwe_distance, columns = ['I', 'posdis', 'ignore']).drop(columns = ['ignore']).assign(Location='strong')\n",
    "    cdf = pd.concat([avg_df, weak_df])#, strong_df])    \n",
    "    cdf = cdf[cdf['posdis']<=5].copy()\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    ax = sns.boxplot(x=\"posdis\", y='I',hue=\"Location\", data=cdf)     #hue=\"Letter\",\n",
    "    plt.show()\n",
    "\n",
    "boxplot_posdis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef6dcf-f9cc-49c9-aac8-5f3028f00cf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7f603-c90c-44aa-abdf-17d1a898a078",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.541929Z",
     "iopub.status.idle": "2023-08-24T21:41:10.542177Z",
     "shell.execute_reply": "2023-08-24T21:41:10.542073Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.542061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avg_df.boxplot(column = ['I'], by = 'posdis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c53f1-8ec6-4f39-b60b-f528a61b49a6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.543071Z",
     "iopub.status.idle": "2023-08-24T21:41:10.543331Z",
     "shell.execute_reply": "2023-08-24T21:41:10.543212Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.543201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc9087-e4a3-4587-82bc-e851a4b6b7fa",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.544661Z",
     "iopub.status.idle": "2023-08-24T21:41:10.544925Z",
     "shell.execute_reply": "2023-08-24T21:41:10.544816Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.544804Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model(X[0:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d853449-b783-4087-8d90-c4c0c413fde0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.546324Z",
     "iopub.status.idle": "2023-08-24T21:41:10.546898Z",
     "shell.execute_reply": "2023-08-24T21:41:10.546644Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.546620Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[x for x in testlist_text if 'Although he wrote' in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95911c8d-5b2c-4b78-a774-0eaaf4a905b0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.548689Z",
     "iopub.status.idle": "2023-08-24T21:41:10.549005Z",
     "shell.execute_reply": "2023-08-24T21:41:10.548887Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.548874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_mode = 1 # [norm of softmax ]\n",
    "\n",
    "\n",
    "# TODO: Fix the prediction fn. Might have to incorporate\n",
    "#  target variable to get the logit for the target variable instead of max\n",
    "predict_fn = llm_helper.get_prediction_fn(model, pred_mode = pred_mode)\n",
    "\n",
    "\n",
    "\n",
    "torch.no_grad()\n",
    "\n",
    "obj = RegressionGame(X = X[0:k], y=None, function = predict_fn, transform = torch.as_tensor)\n",
    "\n",
    "X_samp = X[k:(N+k)]\n",
    "\n",
    "shapley_values = np.empty((0, X.shape[1]))\n",
    "partial_residuals = np.empty((0, X.shape[1]))\n",
    "games = np.empty((0, 2 ** X.shape[1]))\n",
    "\n",
    "print(\"SHape\")\n",
    "print(X.shape[1])\n",
    "\n",
    "print(\"  ..ok!\")\n",
    "print(\"Generating explanations..\")\n",
    "\n",
    "for i in range(0, N):\n",
    "    example_row = X_samp[i,:].reshape((1,X_samp.shape[1]))\n",
    "    game = obj.getKernelSHAPGame(example_row)\n",
    "    games = np.append(games, game.reshape((1,game.shape[0])), axis = 0)\n",
    "    results, residualGame, origGame = getShapleyProjection(game)\n",
    "    shapley_values = np.append(shapley_values,\n",
    "                               np.array([np.flip(results[-1])]), axis=0)\n",
    "    partial_residuals = np.append(partial_residuals,\n",
    "                                  np.array([np.flip(norm(residualGame, axis = 0)/norm(origGame, axis = 0))]), axis = 0)\n",
    "    print(\"%s/%s samples done.\" % (i+1, N))\n",
    "    \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        pd.DataFrame(X_samp).to_csv('data/llm_input.csv')\n",
    "        pd.DataFrame(shapley_values).to_csv('data/llm_shapley_values.csv')\n",
    "        pd.DataFrame(partial_residuals).to_csv('data/llm_partial_residuals.csv')\n",
    "\n",
    "print(\" Explanations saved to data/llm_*.csv!\")\n",
    "\n",
    "pd.DataFrame(X_samp).to_csv('data/llm_input.csv')\n",
    "pd.DataFrame(shapley_values).to_csv('data/llm_shapley_values.csv')\n",
    "pd.DataFrame(partial_residuals).to_csv('data/llm_partial_residuals.csv')\n",
    "\n",
    "\"\"\"\n",
    "TODO: \n",
    "1. How to get base line features for the text generation process? \n",
    "2. What is the appropriate metric for shapley score. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22e10c-cfff-4263-bc32-f15b5e366605",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.549971Z",
     "iopub.status.idle": "2023-08-24T21:41:10.550223Z",
     "shell.execute_reply": "2023-08-24T21:41:10.550119Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.550108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85b61f-b7d2-4053-b597-712cdac20257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8404e0-d4f3-4fad-b020-58ad8f706e8f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.551530Z",
     "iopub.status.idle": "2023-08-24T21:41:10.551781Z",
     "shell.execute_reply": "2023-08-24T21:41:10.551667Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.551656Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_fn = llm_helper.get_prediction_fn(model, pred_mode = 1)\n",
    "\n",
    "\n",
    "\n",
    "# torch.no_grad()\n",
    "\n",
    "\n",
    "# # obj = RegressionGame(X = X[0:k], y=y[0:k], function = predict_fn, transform = torch.as_tensor)\n",
    "# obj = RegressionGame(X = X[0:k], y=None, function = predict_fn, transform = torch.as_tensor)\n",
    "\n",
    "# X_samp = X[k:(N+k)]\n",
    "\n",
    "# shapley_values = np.empty((0, X.shape[1]))\n",
    "# partial_residuals = np.empty((0, X.shape[1]))\n",
    "# games = np.empty((0, 2 ** X.shape[1]))\n",
    "\n",
    "# print(\"SHape\")\n",
    "# print(X.shape[1])\n",
    "\n",
    "# print(\"  ..ok!\")\n",
    "# print(\"Generating explanations..\")\n",
    "\n",
    "# for i in range(0, N):\n",
    "#     example_row = X_samp[i,:].reshape((1,X_samp.shape[1]))\n",
    "#     game = obj.getKernelSHAPGame(example_row)\n",
    "#     games = np.append(games, game.reshape((1,game.shape[0])), axis = 0)\n",
    "#     results, residualGame, origGame = getShapleyProjection(game)\n",
    "#     shapley_values = np.append(shapley_values,\n",
    "#                                np.array([np.flip(results[-1])]), axis=0)\n",
    "#     partial_residuals = np.append(partial_residuals,\n",
    "#                                   np.array([np.flip(norm(residualGame, axis = 0)/norm(origGame, axis = 0))]), axis = 0)\n",
    "#     print(\"%s/%s samples done.\" % (i+1, N))\n",
    "\n",
    "# print(\" Explanations saved to data/llm_*.csv!\")\n",
    "\n",
    "# pd.DataFrame(X_samp).to_csv('data/llm_input.csv')\n",
    "# pd.DataFrame(shapley_values).to_csv('data/llm_shapley_values.csv')\n",
    "# pd.DataFrame(partial_residuals).to_csv('data/llm_partial_residuals.csv')\n",
    "\n",
    "# \"\"\"\n",
    "# TODO: \n",
    "# 1. How to get base line features for the text generation process? \n",
    "# 2. What is the appropriate metric for shapley score. \n",
    "# 3. issue here, check why I was not able to make k = 40. model is probably not able to take more than 25. Check\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbff028-32d9-4cbc-94d3-9574243e1d06",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.552643Z",
     "iopub.status.idle": "2023-08-24T21:41:10.552897Z",
     "shell.execute_reply": "2023-08-24T21:41:10.552793Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.552780Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapley_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a1e48-e0a0-4692-8905-7a8322da0dc9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-08-24T21:41:10.554039Z",
     "iopub.status.idle": "2023-08-24T21:41:10.554280Z",
     "shell.execute_reply": "2023-08-24T21:41:10.554178Z",
     "shell.execute_reply.started": "2023-08-24T21:41:10.554167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9c1e8-b53f-4f3e-bc3c-4d579f6a6f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
